---
title: "Exploratory Analysis"
author: "Kyle Dixon, Mia Li, Caitlin Hennessey"
date: "9/4/2020"
output: html_document
---

## Project Topic: 
## Analyzing the impact of news and social media sentiment on stock prices.

### 1 - General review of raw data 
#### 1.1 Load and clean data
```{r initialize, include = F}
## Load libraries
library(bit64)
library(caret)
library(corrplot)
library(e1071)
library(data.table)
library(dplyr)
library(ggplot2)
library(rvest)
library(stringr)

knitr::opts_chunk$set(echo = T)
```

```{r clean_data}
## Load raw data into the global environment
tsla <- base::data.frame(data.table::fread("1 - Data/Tesla/tsla.csv", na.strings = c("#N/A N/A", "#N/A Invalid Field", "#N/A Requesting Data...")), stringsAsFactors = F)
goog <- base::data.frame(data.table::fread("1 - Data/Google/goog.csv", na.strings = c("#N/A N/A", "#N/A Invalid Field", "#N/A Requesting Data...")), stringsAsFactors = F)

## Clean raw data
tsla <- tsla %>% dplyr::select_if(~sum(!is.na(.)) > 0) %>% # Drop columns that contain only NAs
  dplyr::filter(!is.na(PX_LAST)) # Drop rows where PX_LAST is NA
goog <- goog %>% dplyr::select_if(~sum(!is.na(.)) > 0) %>% # Drop columns that contain only NAs
  dplyr::filter(!is.na(PX_LAST)) # Drop rows where PX_LAST is NA

## Format date columns
tsla$Date <- as.Date(tsla$Date, "%m/%d/%Y")
goog$Date <- as.Date(goog$Date, "%m/%d/%Y")
```

#### 1.2 Visualize data
```{r plot_data}
## Plot prices versus time
tsla_plot <- ggplot2::ggplot() + 
  ggplot2::geom_line(data = tsla, ggplot2::aes(x = Date, y = PX_LAST)) + ggplot2::ggtitle("TSLA")
goog_plot <- ggplot2::ggplot() + 
  ggplot2::geom_line(data = goog, ggplot2::aes(x = Date, y = PX_LAST)) + ggplot2::ggtitle("GOOG") + 
  ggplot2::scale_x_date(limits = c(base::min(tsla$Date), base::max(tsla$Date))) # Align the x-axis with TSLA
gridExtra::grid.arrange(tsla_plot, goog_plot)
```

### 2 - Data Preprocessing
#### 2.1 Count the number of Missing Values in each feature

```{r numNA, warning=FALSE}
require(dplyr)
totalRowTsla = base::nrow(tsla)
totalColTsla = base::ncol(tsla)

naTsla       = tsla %>%
               base::sapply(.,function(y) base::sum(base::length(base::which(base::is.na(y))))) %>%
               base::as.data.frame() %>%
               tibble::rownames_to_column(., "Features")
              

totalRowGoog = base::nrow(goog)
totalColGoog = base::ncol(goog)

naGoog       = goog %>%
               base::sapply(.,function(y) base::sum(base::length(base::which(base::is.na(y))))) %>%
               base::as.data.frame() %>%
               tibble::rownames_to_column(., "Features")
```

>__Tesla__

Total number of rows: `r totalRowTsla`

Total number of columns: `r totalColTsla`


```{r naTslaKnit, echo=FALSE}

knitr::kable(
 naTsla,
 col.names = c("Features","Number of Missing Values"),
 caption   = "Table 2-1: The number of Missing Values - Tesla",
 align     = "lccrr"
)
```

>__Google__

Total number of rows: `r totalRowGoog`

Total number of columns: `r totalColGoog`


```{r naGoogKnit, echo=FALSE}
knitr::kable(
 naGoog,
 col.names = c("Features","Number of Missing Values"),
 caption   = "Table 2-2: The number of Missing Values - Google",
 align     = "lccrr"
)
```


#### 2.2 Data Transformation 

##### 2.2.1 Data Filtering - Tesla
Features with less than 100 missing values will be included by deleting corresponding rows with missing values instead.
```{r filterTsla, warning=FALSE}
#Select a subset of original data for data transformation and PCA
tslaSelected = tsla %>%
  .[,(which(naTsla[,2] < 100))] %>%
  .[!(apply(.,1, function(y){any(is.na(y))})),] %>%
  .[,-1] #remove dates in the first column

#Apply Log-transformation on tslaSelected$TURNOVER to avoid producing NaN in skewness computation
tslaSelected$TURNOVER = log(tslaSelected$TURNOVER)
base::names(tslaSelected) [6]= "log(TURNOVER)"
```

##### 2.2.2 Calculate Skewness and Kurtosis - Tesla
```{r skewnessTsla, warning=FALSE}
require(e1071)
skewnessTsla = apply(tslaSelected,2,e1071::skewness)
kurtosisTsla   = apply(tslaSelected,2,e1071::kurtosis)
```

```{r skewnessTslaTable, warning=FALSE, echo=FALSE}
skewnessTsla = skewnessTsla %>%
               as.data.frame() %>%
               tibble::rownames_to_column(., "features")

knitr::kable(
 skewnessTsla,
 col.names = c("Features","Skewness"),
 caption   = "Table 2-3: Skewness of Selected Features - Tesla",
 align     = "lccrr"
)

kurtosisTsla = kurtosisTsla %>%
               as.data.frame() %>%
               tibble::rownames_to_column(., "features")

knitr::kable(
 kurtosisTsla,
 col.names = c("Features","Kurtosis"),
 caption   = "Table 2-4: Kurtosis of Selected Features - Tesla",
 align     = "lccrr"
)
```

##### 2.2.3 Data Transformation - Tesla
```{r dataTransTsla, warning=FALSE}
#Transform the selected data
(tslaTrans = caret::preProcess(tslaSelected, method = c("BoxCox","center","scale","pca")))

#Apply the transformations("BoxCox","center","scale","pca")
tslaPCA = stats::predict(tslaTrans,tslaSelected) #Outputs are PCA components
utils::head(tslaPCA)

#Summary of PCA results
tslaTrans_     = caret::preProcess(tslaSelected, method = c("BoxCox","center","scale"))
propVarTslaPCA = tslaSelected %>%
                 stats::predict(tslaTrans_,.) %>%
                 stats::prcomp() %>%
                 summary()
propVarTslaPCA #proportion of variance explained by each PCs
```

#
#### Fig. 2.1 Density Plots  - Tesla
```{r dataPlotsTsla, echo=FALSE}
transformedTsla = predict(tslaTrans_,tslaSelected)
par(mfrow=c(2,2))
for (i in 1:ncol(tslaSelected)) {
  hist(tslaSelected[,i], breaks=20, freq=FALSE, 
       main = paste("Density Plot - ",names(tslaSelected)[i],                            "(Before)"),xlab = " ")
  hist(transformedTsla[,i], breaks=20, freq=FALSE, 
       main = paste("Density Plot - ",names(transformedTsla)[i],
                          "(After)"),xlab = " ")
}
```


##### 2.2.4 Data Filtering - Google
Features with less than 100 missing values will be included by deleting corresponding rows with missing values instead.
```{r filterGoog, warning=FALSE}
#Select a subset of original data for data transformation and PCA
googSelected = goog %>%
  .[,(which(naGoog[,2] < 100))] %>%
  .[!(apply(.,1, function(y){any(is.na(y))})),] %>%
  .[,-1] #remove dates in the first column

#Apply Log-transformation on googSelected$TURNOVER
googSelected$TURNOVER = base::log(googSelected$TURNOVER)
base::names(googSelected) [6]= "log(TURNOVER)"
```


##### 2.2.5 Calculate Skewness and Kurtosis - Google
```{r skewnessGoog, warning=FALSE}
#require(e1071)
skewnessGoog = apply(googSelected,2,e1071::skewness)
kurtosisGoog = apply(googSelected,2,e1071::kurtosis)
```

```{r skewnessGoogTable, warning=FALSE, echo=FALSE}
skewnessGoog = skewnessGoog%>%
               as.data.frame() %>%
               tibble::rownames_to_column(., "features")

knitr::kable(
 skewnessGoog,
 col.names = c("Features","Skewness"),
 caption   = "Table 2-5: Skewness of Selected Features - Google",
 align     = "lccrr"
)


kurtosisGoog = kurtosisGoog %>%
               as.data.frame() %>%
               tibble::rownames_to_column(., "features")

knitr::kable(
 kurtosisGoog,
 col.names = c("Features","Kurtosis"),
 caption   = "Table 2-6: Kurtosis of Selected Features - Google",
 align     = "lccrr"
)

```


##### 2.2.6 Data Transformation - Google
```{r dataTransGoog, warning=FALSE}
#Transform the selected data
(googTrans = caret::preProcess(googSelected, method = c("BoxCox","center","scale","pca")))

#Apply the transformations("BoxCox","center","scale","pca")
googPCA = stats::predict(googTrans,googSelected) #Outputs are PCA components
utils::head(googPCA)

#Summary of PCA results
(googTrans_    = caret::preProcess(googSelected, method = c("BoxCox","center","scale")))
propVarGoogPCA = googSelected %>%
                 stats::predict(googTrans_,.) %>%
                 stats::prcomp() %>%
                 summary()
propVarGoogPCA #proportion of variance explained by each PCs
```

#
#### Fig. 2.2 Density Plots  - Google
```{r dataPlotsGoog, echo=FALSE}
transGoog = predict(googTrans_,googSelected)
par(mfrow=c(2,2))
for (i in 1:ncol(googSelected)) {
  hist(googSelected[,i], breaks=20, freq=FALSE, 
       main = paste("Density Plot - ",names(googSelected)[i], "(After)"),
  xlab = " ")
  hist(transGoog[,i], breaks=20, freq=FALSE, 
       main = paste("Density Plot - ",names(transGoog)[i], "(Before)"),
  xlab = " ")
}
```



### 2.3 Correlation Analysis 
>__Tesla__

```{r findCorrelationTsla, warning=FALSE}
library(corrplot)
corrTsla     = stats::cor(tslaSelected)
corrplot::corrplot(corrTsla, order="hclust")
```

#### Fig. 2.3 Correlations between features - Tesla

>__Google__

```{r findCorrelationGoog, warning=FALSE}
#library(corrplot)
corrGoog     = stats::cor(googSelected)
corrplot::corrplot(corrGoog, order="hclust")
```

#### Fig. 2.4 Correlations between features - Google


### 3 - News Scraping
```{r news_scrape}
## Define search strings on Google News
tsla_search <- read_html("https://news.google.com/search?q=TSLA%20when%3A10y&hl=en-US&gl=US&ceid=US%3Aen")
goog_search <- read_html("https://news.google.com/search?q=GOOG%20when%3A10y&hl=en-US&gl=US&ceid=US%3Aen")

## Extract headlines
tsla_headlines <- tsla_search %>% 
  rvest::html_nodes("article") %>% rvest::html_text("span") %>%
  stringr::str_split("(?<=[a-z0-9!?\\.])(?=[A-Z])") # Clean strings
tsla_headlines <- base::sapply(tsla_headlines, function(x) x[1]) # Extract only the first elements

goog_headlines <- goog_search %>% 
  rvest::html_nodes("article") %>% rvest::html_text("span") %>%
  stringr::str_split("(?<=[a-z0-9!?\\.])(?=[A-Z])") # Clean strings
goog_headlines <- base::sapply(goog_headlines, function(x) x[1]) # Extract only the first elements

## Extract the time since the headline
tsla_time <- tsla_search %>% rvest::html_nodes("div article div div time") %>% rvest::html_text()
goog_time <- goog_search %>% rvest::html_nodes("div article div div time") %>% rvest::html_text()

## Create data frame
tsla_news <- base::data.frame("Date" = tsla_time, "Headline" = tsla_headlines, stringsAsFactors = F)
goog_news <- base::data.frame("Date" = goog_time, "Headline" = goog_headlines, stringsAsFactors = F)

## Format date columns
for (row in 1:base::nrow(tsla_news)) {
  # If news was hours ago, set date as today
  if (base::grepl("hours ago", tsla_news[row, ]$Date)) {tsla_news[row, ]$Date <- Sys.Date()}
  #
  else if (base::grepl("Yesterday", tsla_news[row, ]$Date)) {tsla_news[row, ]$Date <- Sys.Date() - 1}
  # If news was days ago, set date as today minus the number of days
  else if (base::grepl("days ago", tsla_news[row, ]$Date)) {tsla_news[row, ]$Date <- Sys.Date() - base::as.numeric(base::substr(tsla_news[row, ]$Date, 1, 1))}
  # If news has a date, format it as a date
  else {tsla_news[row, ]$Date <- base::as.Date(paste0(tsla_news[row, ]$Date, " 2020"), "%b %d %Y")}
}
tsla_news$Date <- base::as.Date(base::as.numeric(tsla_news$Date), origin = "1970-01-01")

for (row in 1:base::nrow(goog_news)) {
  # If news was hours ago, set date as today
  if (base::grepl("hours ago", goog_news[row, ]$Date)) {goog_news[row, ]$Date <- Sys.Date()}
  #
  else if (base::grepl("Yesterday", goog_news[row, ]$Date)) {goog_news[row, ]$Date <- Sys.Date() - 1}
  # If news was days ago, set date as today minus the number of days
  else if (base::grepl("days ago", goog_news[row, ]$Date)) {goog_news[row, ]$Date <- Sys.Date() - base::as.numeric(base::substr(goog_news[row, ]$Date, 1, 1))}
  # If news has a date, format it as a date
  else {goog_news[row, ]$Date <- base::as.Date(paste0(goog_news[row, ]$Date, " 2020"), "%b %d %Y")}
}
goog_news$Date <- base::as.Date(base::as.numeric(goog_news$Date), origin = "1970-01-01")
```

#### 4 - Linear Regression
##### 4.1 Data Structure
```{r dataStructure}
#Tsla
str(tslaSelected)
sapply(tslaSelected,function(x){length(unique(x))})

#Goog
str(googSelected)
sapply(googSelected,function(x){length(unique(x))})
```

All `r ncol(tslaSelected)` selected variables in Tesla stock data are quantitative.  
All `r ncol(googSelected)` selected variables in Google stock data are quantitative.

```{r highCorr}
#Recall highly correlated variables before making changes
highCorrTsla = tslaSelected %>%
  cor %>%
  caret::findCorrelation(.8, names = TRUE)
highCorrTsla
corrplot::corrplot(cor(tslaSelected), order="hclust")

highCorrGoog = googSelected %>%
  cor %>%
  caret::findCorrelation(.8, names = TRUE)
highCorrGoog
corrplot::corrplot(cor(googSelected), order="hclust")
```

Highly correlated features in Tesla data set are : `r highCorrTsla`.
Highly correlated features in Google data set are : `r highCorrGoog`.

Instead of removing all these features, try to calculate new quantities based on them and check the correlations of data with new features added. 

```{r newVar}
###TSLA
tslaTemp = tslaSelected %>% 
  dplyr::mutate(
         PP = (PX_HIGH + PX_LOW + PX_LAST)/3, #Pivot Point
         R1 = 2 * PP - PX_LOW,                #First resistance
         S1 = 2 * PP - PX_HIGH,               #First support
         R2 = PP + (PX_HIGH - PX_LOW),
         S2 = PP - (PX_HIGH - PX_LOW),
         R3 = PX_HIGH + 2 * (PP - PX_LOW),
         S3 = PX_LOW - 2 * (PX_HIGH - PP),
         diffHighLow  = PX_HIGH - PX_LOW,
         diffLastOpen = PX_LAST - PX_OPEN,
         stockReturn  = (PX_LAST - PX_OPEN)/PX_OPEN,
         obv = rep(0, nrow(tslaSelected))
         )
  
highCorrTsla = tslaTemp %>%
  cor %>%
  caret::findCorrelation(.8, names = TRUE)
highCorrTsla

corrplot::corrplot(cor(tslaTemp), order="hclust")

###GOOG


```
Add lagged features to run linear regression analysis with past values. We'll fit time series models later. This can be a fairly good alternative for TS models, which saves us from running the stationarity test for each feature. 

```{r addVariables}
addLaggedVarF = function(feature, laggedVar, featureName,lagSize ) {
  for (lags in 1:lagSize) { 
    newColName = paste0(featureName, lags)
    laggedVar = laggedVar %>%
      dplyr::mutate(!!sym(newColName) := dplyr::lag(feature, n = lags))
  }
  return(laggedVar)
}

#Create lagged features. Only retain the lagged features with correlation less than 0.8
tslaLagged = addLaggedVarF(tslaTemp$PX_VOLUME, tslaTemp, "VOLUME",10)
tslaLagged = addLaggedVarF(tslaLagged$diffHighLow, tslaLagged, "diffHighLow",10)
tslaLagged = addLaggedVarF(tslaLagged$diffLastOpen, tslaLagged, "diffLastOpen",10)
tslaLagged = addLaggedVarF(tslaLagged$PX_LAST, tslaLagged, "PX_LAST",1) #used to calculate OBV
#tslaLagged = addLaggedVarF(tslaLagged$obv, tslaLagged, "obv",1) #used to calculate OBV

#Remove rows with NAs
tslaLaggedRemoveNA = tslaLagged %>%
  filter(.,!is.na(tslaLagged$VOLUME10)) 
names(tslaLaggedRemoveNA)
```

On Balance Volume is calculated by adding the day's volume to a cumulative total when the security's price closes up, and subtracting the day's volume when the security's price closes down.

If today's close is greater than yesterday's close then:
OBV = Yesterday’s OBV + Today’s Volume

If today’s close is less than yesterday’s close then:
OBV = Yesterday’s OBV – Today’s Volume

If today’s close is equal to yesterday’s close then:
OBV = Yesterday’s OBV
```{r OnBalanceVolume}
tslaTemp2 =  tslaLaggedRemoveNA %>%
  if (PX_LAST1 < PX_LAST){
      mutate_at(obv = lag(obv) + PX_VOLUME)
    } else if (PX_LAST1 > PX_LAST){ 
      mutate_at(obv = lag(obv) - PX_VOLUME)
    } else {
      mutate_at(obv = lag(obv))
    }

obvF = function(yesterdayVolume, todayVolume, yesterdayPX_LAST, todayPX_LAST, obv){
  if (yesterdayPX_LAST < todayPX_LAST){
      mutate_at(obv = lag(obv) + todayVolume)
    } else if (yesterdayPX_LAST > todayPX_LAST){ 
      mutate_at(obv = lag(obv) - todayVolume)
    } else {
      mutate_at(obv = lag(obv))
    }
}

#Chaikin money flow


#Klinger Oscillator

###GOOG
```

```{r removeHighCorr}
highCorrTsla = tslaLaggedRemoveNA %>%
  cor %>%
  caret::findCorrelation(.8, names = TRUE)
highCorrTsla #print highly correlated features

tslaLaggedRemoveCorr = select(tslaLaggedRemoveNA, -highCorrTsla)

corrplot::corrplot(cor(tslaLaggedRemoveCorr), order="hclust")


#Calculate the 1st difference of features [feature_t - feature_(t-1)]

tslaLaggedHighCorr =select(tslaLaggedRemoveNA, highCorrTsla)

highCorrTslaDiff = tslaLaggedHighCorr %>%
  apply(., 2, function(x) {diff(x)})%>%
  dplyr::as_tibble(.) %>%
  cor %>%
  caret::findCorrelation(.8, names = TRUE)
highCorrTslaDiff


tslaDiffRemoveCorr = select(tslaLaggedHighCorr, -highCorrTslaDiff)
names(tslaDiffRemoveCorr) = paste0("diff",names(tslaDiffRemoveCorr)) 

tslaReg = cbind(tslaLaggedRemoveCorr,tslaDiffRemoveCorr) #Features for Linear Regression
corrplot::corrplot(cor(tslaReg), order="hclust")

```




```{r dummyVar}
....

```




##### 4.2 Google


##### 4.x Potential Problems
###### 4.x.1 correlation of error term(residuals)
###### 4.x.2 outliers


#### 5 - Time Series Analysis

```{r timeSeriesAnalysis}
#Check stationarity
cuberoot=function(x){
    x=sign(x)*abs(x)^(1/3)
  }
require(graphics)
par(mfrow=c(2,2))
tslaTsPlot = tslaReg %>% 
  apply(., 2, function(x) {diff(diff(cuberoot(x)))}) %>%
  apply(., 2, ts.plot)

tslaACF = tslaReg %>% 
  apply(., 2, function(x) {diff(diff(cuberoot(x)))}) %>%
  apply(., 2, acf)

```

#### 6 - K-means clustering (Unsupervised learning)
Goal: Use clustering techniques to figure out whether the current pool of features can supply enough information to build a classifier for stock price trend. 

Step 1: 
Response variable is defined as a qualitative variable: 1 - Increased stock price (PX_LAST), 0 - Decreased stock price;
Cluster features into two groups. (K=2)

Step 2:
Calculate the percentage of increased stock prices in each cluster. (K=2)

Conclusion: If the percentage of one cluster is much larger than the other one, a classifier based on all used features should have a fairly good performance. 


```{r K-meansClustering}


```