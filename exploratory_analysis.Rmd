---
title: "Exploratory Analysis"
author: "Kyle Dixon, Mia Li, Caitlin Hennessey"
date: "9/4/2020"
output: html_document
---

## Project Topic:
## Analyzing the impact of news and social media sentiment on stock prices.

### 1 - General review of raw data
#### 1.1 Load and clean data
```{r initialize, include = F}
## Load libraries
library(bit64) # To read raw data
library(caret)
library(corrplot)
library(data.table) # Faster CSV reader than base
library(dplyr)
library(e1071)
library(ggplot2) # General plotting
library(newsanchor) # News API scrape
library(readxl)
library(rtweet) # Twitter scrape
library(rvest)
library(stringr) # Goggle news scrape
library(tau)
library(textdata)
library(plot3D)   #PCs plot
library(RcppRoll) #CMF
library(TTR)      #Calculate Indicators: EMA,SMA,RSI
library(leaps)    #Stepwise Model Selection
library(zoo) # Interpolation
library(elasticnet) #ridge, lasso, elastic net regression

knitr::opts_chunk$set(echo = T)
```

```{r clean_data}
## Load raw data into the global environment
tsla <- base::data.frame(data.table::fread("1 - Data/Tesla/tsla.csv", na.strings = c("#N/A N/A", "#N/A Invalid Field", "#N/A Requesting Data...")), stringsAsFactors = F)
goog <- base::data.frame(data.table::fread("1 - Data/Google/goog.csv", na.strings = c("#N/A N/A", "#N/A Invalid Field", "#N/A Requesting Data...")), stringsAsFactors = F)

## Clean raw data
tsla <- tsla %>% dplyr::select_if(~sum(!is.na(.)) > 0) %>% # Drop columns that contain only NAs
  dplyr::filter(!is.na(PX_LAST)) # Drop rows where PX_LAST is NA
goog <- goog %>% dplyr::select_if(~sum(!is.na(.)) > 0) %>% # Drop columns that contain only NAs
  dplyr::filter(!is.na(PX_LAST)) # Drop rows where PX_LAST is NA

## Format date columns
tsla$Date <- as.Date(tsla$Date, "%m/%d/%Y")
goog$Date <- as.Date(goog$Date, "%m/%d/%Y")
```

#### 1.2 Visualize data

```{r plot_data}
## Plot prices versus time
tsla_plot <- ggplot2::ggplot() +
  ggplot2::geom_line(data = tsla, ggplot2::aes(x = Date, y = PX_LAST)) + ggplot2::ggtitle("TSLA")
goog_plot <- ggplot2::ggplot() +
  ggplot2::geom_line(data = goog, ggplot2::aes(x = Date, y = PX_LAST)) + ggplot2::ggtitle("GOOG") +
  ggplot2::scale_x_date(limits = c(base::min(tsla$Date), base::max(tsla$Date))) # Align the x-axis with TSLA
gridExtra::grid.arrange(tsla_plot, goog_plot)
```

### 2 - Data Preprocessing (Missingness and Collinearity)
#### 2.1 Data Interpolation

As we have both daily and quarterly updated features aligned in the original data set, quarterly updated features contains mainly missing values. Instead of deleting those features, missing values are interpolated between quarters.   

``` {r interpolation}
## TSLA
## Run linear interpolations
tmp <- zoo::na.approx(tsla[, base::c(14:22, 24:26)])

## Create empty matrix as placeholder for non-interpolated rows
row_diff <- base::nrow(tsla) - base::nrow(tmp)
empty <- base::matrix(base::rep(NA, row_diff * base::ncol(tmp)), nrow = row_diff, ncol = base::ncol(tmp))
base::colnames(empty) <- base::colnames(tmp)  
tmp <- base::rbind(tmp, empty)

## Replace original data with interpolation
tsla[, base::c(14:22, 24:26)] <- tmp

## GOOG
## Run linear interpolations
tmp <- zoo::na.approx(goog[, 11:23])

## Create empty matrix as placeholder for non-interpolated rows
row_diff <- base::nrow(goog) - base::nrow(tmp)
empty <- base::matrix(base::rep(NA, row_diff * base::ncol(tmp)), nrow = row_diff, ncol = base::ncol(tmp))
base::colnames(empty) <- base::colnames(tmp)  
tmp <- base::rbind(tmp, empty)

## Replace original data with interpolation
goog[, 11:23] <- tmp
```

#### 2.2 Count the number of Missing Values in each feature

```{r numNA, warning=FALSE}
require(dplyr)
totalRowTsla = base::nrow(tsla)
totalColTsla = base::ncol(tsla)

naTsla       = tsla %>%
               base::sapply(.,function(y) base::sum(base::length(base::which(base::is.na(y))))) %>%
               base::as.data.frame() %>%
               tibble::rownames_to_column(., "Features")


totalRowGoog = base::nrow(goog)
totalColGoog = base::ncol(goog)

naGoog       = goog %>%
               base::sapply(.,function(y) base::sum(base::length(base::which(base::is.na(y))))) %>%
               base::as.data.frame() %>%
               tibble::rownames_to_column(., "Features")
```

>__Tesla__

Total number of rows: `r totalRowTsla`

Total number of columns: `r totalColTsla`


```{r naTslaKnit, echo=FALSE}
knitr::kable(
 naTsla,
 col.names = c("Features","Number of Missing Values"),
 caption   = "Table 2-1: The number of Missing Values - Tesla",
 align     = "lccrr"
)
```

>__Google__

Total number of rows: `r totalRowGoog`

Total number of columns: `r totalColGoog`


```{r naGoogKnit, echo=FALSE}
knitr::kable(
 naGoog,
 col.names = c("Features","Number of Missing Values"),
 caption   = "Table 2-2: The number of Missing Values - Google",
 align     = "lccrr"
)
```


#### 2.3 Data Transformation

##### 2.3.1 Data Filtering - Tesla
Features with less than 100 missing values will be included by deleting corresponding rows with missing values instead.
```{r filterTsla, warning=FALSE}
#Select a subset of original data for data transformation and PCA
tslaSelected = tsla %>%
  .[,(which(naTsla[,2] < 100))] %>%
  .[!(apply(.,1, function(y){any(is.na(y))})),] %>%
  .[,-1] #remove dates in the first column

#Apply Log-transformation on tslaSelected$TURNOVER to avoid producing NaN in skewness computation
tslaSelected$TURNOVER = log(tslaSelected$TURNOVER)
base::names(tslaSelected) [6]= "log(TURNOVER)"
```

##### 2.3.2 Calculate Skewness and Kurtosis - Tesla
```{r skewnessTsla, warning=FALSE}
require(e1071)
skewnessTsla = apply(tslaSelected,2,e1071::skewness)
kurtosisTsla   = apply(tslaSelected,2,e1071::kurtosis)
```

```{r skewnessTslaTable, warning=FALSE, echo=FALSE}
skewnessTsla = skewnessTsla %>%
               as.data.frame() %>%
               tibble::rownames_to_column(., "features")

knitr::kable(
 skewnessTsla,
 col.names = c("Features","Skewness"),
 caption   = "Table 2-3: Skewness of Selected Features - Tesla",
 align     = "lccrr"
)

kurtosisTsla = kurtosisTsla %>%
               as.data.frame() %>%
               tibble::rownames_to_column(., "features")

knitr::kable(
 kurtosisTsla,
 col.names = c("Features","Kurtosis"),
 caption   = "Table 2-4: Kurtosis of Selected Features - Tesla",
 align     = "lccrr"
)
```

##### 2.3.3 Data Transformation - Tesla
```{r dataTransTsla, warning=FALSE}
#Transform the selected data
(tslaTrans = caret::preProcess(tslaSelected, method = c("BoxCox","center","scale","pca")))

#Apply the transformations("BoxCox","center","scale","pca")
tslaPCA = stats::predict(tslaTrans,tslaSelected) #Outputs are PCA components
utils::head(tslaPCA)

#Summary of PCA results
tslaTrans_     = caret::preProcess(tslaSelected, method = c("BoxCox","center","scale"))
propVarTslaPCA = tslaSelected %>%
                 stats::predict(tslaTrans_,.) %>%
                 stats::prcomp() %>%
                 summary()
propVarTslaPCA #proportion of variance explained by each PCs

#Check extreme observations via PCA

ggplot(data = tslaPCA)+
  geom_point(aes(x = PC1, y= PC2))

ggplot(data = tslaPCA)+
  geom_point(aes(x = PC3, y= PC2))

ggplot(data = tslaPCA)+
  geom_point(aes(x = PC1, y= PC3))


plot3D::scatter3D(tslaPCA$PC1, tslaPCA$PC2, tslaPCA$PC3, pch = 16,  
                  theta = 20, phi = 20, main = "PCA - Tesla", xlab = "PC1", 
                  ylab ="PC2", zlab = "PC3", bty = "g", ticktype = "detailed")

```

#### Fig. 2.1 Density Plots  - Tesla
```{r dataPlotsTsla, echo=FALSE}
transformedTsla = predict(tslaTrans_,tslaSelected)
par(mfrow=c(2,2))
for (i in 1:ncol(tslaSelected)) {
  hist(tslaSelected[,i], breaks=20, freq=FALSE,
       main = paste("Density Plot - ",names(tslaSelected)[i],                            "(Before)"),xlab = " ")
  hist(transformedTsla[,i], breaks=20, freq=FALSE,
       main = paste("Density Plot - ",names(transformedTsla)[i],
                          "(After)"),xlab = " ")
}
```

##### 2.3.4 Data Filtering - Google
Features with less than 100 missing values will be included by deleting corresponding rows with missing values instead.
```{r filterGoog, warning=FALSE}
#Select a subset of original data for data transformation and PCA
googSelected = goog %>%
  .[,(which(naGoog[,2] < 100))] %>%
  .[!(apply(.,1, function(y){any(is.na(y))})),] %>%
  .[,-1] #remove dates in the first column

#Apply Log-transformation on googSelected$TURNOVER
googSelected$TURNOVER = base::log(googSelected$TURNOVER)
base::names(googSelected) [6]= "log(TURNOVER)"
```

##### 2.3.5 Calculate Skewness and Kurtosis - Google
```{r skewnessGoog, warning=FALSE}
#require(e1071)
skewnessGoog = apply(googSelected,2,e1071::skewness)
kurtosisGoog = apply(googSelected,2,e1071::kurtosis)
```

```{r skewnessGoogTable, warning=FALSE, echo=FALSE}
skewnessGoog = skewnessGoog%>%
               as.data.frame() %>%
               tibble::rownames_to_column(., "features")

knitr::kable(
 skewnessGoog,
 col.names = c("Features","Skewness"),
 caption   = "Table 2-5: Skewness of Selected Features - Google",
 align     = "lccrr"
)


kurtosisGoog = kurtosisGoog %>%
               as.data.frame() %>%
               tibble::rownames_to_column(., "features")

knitr::kable(
 kurtosisGoog,
 col.names = c("Features","Kurtosis"),
 caption   = "Table 2-6: Kurtosis of Selected Features - Google",
 align     = "lccrr"
)

```

##### 2.3.6 Data Transformation - Google
```{r dataTransGoog, warning=FALSE}
#Transform the selected data
(googTrans = caret::preProcess(googSelected, method = c("BoxCox","center","scale","pca")))

#Apply the transformations("BoxCox","center","scale","pca")
googPCA = stats::predict(googTrans,googSelected) #Outputs are PCA components
utils::head(googPCA)

#Summary of PCA results
(googTrans_    = caret::preProcess(googSelected, method = c("BoxCox","center","scale")))
propVarGoogPCA = googSelected %>%
                 stats::predict(googTrans_,.) %>%
                 stats::prcomp() %>%
                 summary()
propVarGoogPCA #proportion of variance explained by each PCs

#Check extreme observations via PCA
ggplot(data = googPCA)+
  geom_point(aes(x = PC1, y= PC2))

ggplot(data = googPCA)+
  geom_point(aes(x = PC3, y= PC2))

ggplot(data = googPCA)+
  geom_point(aes(x = PC1, y= PC3))

plot3D::scatter3D(googPCA$PC1, googPCA$PC2, googPCA$PC3, pch = 16,  
                  theta = 20, phi = 20, main = "PCA - Alphabet", xlab = "PC1", 
                  ylab ="PC2", zlab = "PC3", bty = "g", ticktype = "detailed")

```

#### Fig. 2.2 Density Plots  - Google
```{r dataPlotsGoog, echo=FALSE}
transGoog = predict(googTrans_,googSelected)
par(mfrow=c(2,2))
for (i in 1:ncol(googSelected)) {
  hist(googSelected[,i], breaks=20, freq=FALSE,
       main = paste("Density Plot - ",names(googSelected)[i], "(After)"),
  xlab = " ")
  hist(transGoog[,i], breaks=20, freq=FALSE,
       main = paste("Density Plot - ",names(transGoog)[i], "(Before)"),
  xlab = " ")
}
```

#### 2.4 Correlation Analysis
>__Tesla__

```{r findCorrelationTsla, warning=FALSE}
library(corrplot)
corrTsla     = stats::cor(tslaSelected)
corrplot::corrplot(corrTsla, order="hclust")
```

#### Fig. 2.3 Correlations between features - Tesla

>__Google__

```{r findCorrelationGoog, warning=FALSE}
#library(corrplot)
corrGoog     = stats::cor(googSelected)
corrplot::corrplot(corrGoog, order="hclust")
```

#### Fig. 2.4 Correlations between features - Google

### 3 - News Scraping
#### 3.1 Google News

```{r google_news_scrape}
## Define search strings on Google News
tsla_search <- read_html("https://news.google.com/search?q=TSLA%20when%3A10y&hl=en-US&gl=US&ceid=US%3Aen")
goog_search <- read_html("https://news.google.com/search?q=GOOG%20when%3A10y&hl=en-US&gl=US&ceid=US%3Aen")

## Extract headlines
tsla_headlines <- tsla_search %>%
  rvest::html_nodes("article") %>% rvest::html_text("span") %>%
  stringr::str_split("(?<=[a-z0-9!?\\.])(?=[A-Z])") # Clean strings
tsla_headlines <- base::sapply(tsla_headlines, function(x) x[1]) # Extract only the first elements

goog_headlines <- goog_search %>%
  rvest::html_nodes("article") %>% rvest::html_text("span") %>%
  stringr::str_split("(?<=[a-z0-9!?\\.])(?=[A-Z])") # Clean strings
goog_headlines <- base::sapply(goog_headlines, function(x) x[1]) # Extract only the first elements

## Extract the time since the headline
tsla_time <- tsla_search %>% rvest::html_nodes("div article div div time") %>% rvest::html_text()
goog_time <- goog_search %>% rvest::html_nodes("div article div div time") %>% rvest::html_text()

## Create data frame
tsla_news <- base::data.frame("Date" = tsla_time, "Headline" = tsla_headlines, stringsAsFactors = F)
goog_news <- base::data.frame("Date" = goog_time, "Headline" = goog_headlines, stringsAsFactors = F)

## Format date columns
for (row in 1:base::nrow(tsla_news)) {
  # If news was hours ago, set date as today
  if (base::grepl("hours ago", tsla_news[row, ]$Date)) {tsla_news[row, ]$Date <- Sys.Date()}
  #
  else if (base::grepl("Yesterday", tsla_news[row, ]$Date)) {tsla_news[row, ]$Date <- Sys.Date() - 1}
  # If news was days ago, set date as today minus the number of days
  else if (base::grepl("days ago", tsla_news[row, ]$Date)) {tsla_news[row, ]$Date <- Sys.Date() - base::as.numeric(base::substr(tsla_news[row, ]$Date, 1, 1))}
  # If news has a date, format it as a date
  else {tsla_news[row, ]$Date <- base::as.Date(paste0(tsla_news[row, ]$Date, " 2020"), "%b %d %Y")}
}
tsla_news$Date <- base::as.Date(base::as.numeric(tsla_news$Date), origin = "1970-01-01")

for (row in 1:base::nrow(goog_news)) {
  # If news was hours ago, set date as today
  if (base::grepl("hours ago", goog_news[row, ]$Date)) {goog_news[row, ]$Date <- Sys.Date()}
  #
  else if (base::grepl("Yesterday", goog_news[row, ]$Date)) {goog_news[row, ]$Date <- Sys.Date() - 1}
  # If news was days ago, set date as today minus the number of days
  else if (base::grepl("days ago", goog_news[row, ]$Date)) {goog_news[row, ]$Date <- Sys.Date() - base::as.numeric(base::substr(goog_news[row, ]$Date, 1, 1))}
  # If news has a date, format it as a date
  else {goog_news[row, ]$Date <- base::as.Date(paste0(goog_news[row, ]$Date, " 2020"), "%b %d %Y")}
}
goog_news$Date <- base::as.Date(base::as.numeric(goog_news$Date), origin = "1970-01-01")
```

#### 3.2 News API

```{r news_api_scrape}
## Save the API key
api_key <- "5912dc14a8794fecb766c3972b84e63e"

## Search news for TSLA
tsla_search <- newsanchor::get_headlines_all(query = "Tesla", api_key = api_key)
tsla_news <- base::data.frame("Date" = tsla_search[["results_df"]][["published_at"]], 
                              "Headline" = tsla_search[["results_df"]][["title"]], 
                              stringsAsFactors = F)

## Search news for Google
goog_search <- newsanchor::get_headlines_all(query = "Google", api_key = api_key)
goog_news <- base::data.frame("Date" = goog_search[["results_df"]][["published_at"]], 
                              "Headline" = goog_search[["results_df"]][["title"]], 
                              stringsAsFactors = F)
```

### 4 - Twitter Scraping

```{r twitter_scrape}
## Twitter API keys and secrets
api_key <- "zElIwld33bNohcb28j7si3aBW"
api_secret_key <- "a5yDRmHeFpq2tpqk4u687rAyDuApzUe2ZffYir3disyqa98QGb"
bearer_token <- "AAAAAAAAAAAAAAAAAAAAAH2iGAEAAAAApwFAjq60a%2Ba8gOG2URX2scAHOo0%3DPXz3xx1Z5i8YuqNzRt6wSAINxOkqNrArIESFlnxWCToPHhEcRH"
access_token <- "1271553006522306568-Azxsa0o7nHOCNpvRjRdocbWPYz0D6w"
access_token_secret <- "PwT1GB0Ogj0poevifeUShWB4lEZAODXHWfm8HyXRz2J7e"

## Create Twitter token
twitter_token <- rtweet::create_token(app = "BNC Sentiment Analysis", consumer_key = api_key, consumer_secret = api_secret_key, access_token = access_token, access_secret = access_token_secret)

## Get positive and negative words from Loughran and McDonald
positive_words <- base::data.frame(
  readxl::read_excel("2 - Word Lists/LoughranMcDonald_SentimentWordLists_2018.xlsx",
                     sheet = "Positive", col_names = F))$...1
positive_words <- positive_words$...1
negative_words <- base::data.frame(
  readxl::read_excel("2 - Word Lists/LoughranMcDonald_SentimentWordLists_2018.xlsx",
                     sheet = "Negative", col_names = F))
negative_words <- negative_words$...1

## Loop through tickers and pull tweets
sentiment <- base::data.frame("Ticker" = base::character(),
                              "Date" = base::as.Date(base::character()),
                              "Sentiment" = base::numeric(), stringsAsFactors = F)
for (ticker in c("TSLA", "GOOG")) {
  ## Pull tweets (this will throw an authentication page the first time)
  ticker <- "TSLA"
  tweets <- rtweet::search_tweets(q = base::paste0("$", ticker), n = 100000, include_rts = F,
                                  retryonratelimit = T)

  ## Check sentiment
  tweets$sentiment <- base::as.numeric(NA)
  for(row in 1:base::nrow(tweets)) {
    values <- base::numeric()
    for (word in base::strsplit(tweets[row, ]$text, "[[:punct:] ]")[[1]]) {
      if (word %in% positive_words) {values <- append(values, 1)}
      else if (word %in% negative_words) {values <- append(values, -1)}
      else {values <- append(values, 0)}
    }
  tweets[row, ]$sentiment <- base::sum(values)
  }

  tweets$sentiment <- base::round(stats::weighted.mean(tweets$sentiment,
                                                 w = tweets$favorite_count + tweets$favorite_count),
                            digits = 1)
  tweets$created_at <- base::as.Date(tweets$created_at, "%Y-%m-%d")
  tmp <- tweets %>% dplyr::group_by(created_at) %>% dplyr::summarize("sentiment" = base::sum(sentiment))
  sentiment <- base::append(sentiment, tmp)
}
names(sentiment) <- c("TSLA", "GOOG")

ggplot2::ggplot(sentiment, ggplot2::aes(x = Date, y = Sentiment, color = Ticker)) + ggplot2::geom_line() + ggplot2::ggtitle("Twitter Sentiment Over Time")
```

### 5 - PCA Regression
#### 5.1 Data Preprocessing

Before making changes, let's take a look on the data structures. 
```{r dataStructure}
#Tsla
#str(tslaSelected)
sapply(tslaSelected,function(x){length(unique(x))})

#Goog
#str(googSelected)
sapply(googSelected,function(x){length(unique(x))})
```

All `r ncol(tslaSelected)` selected variables in Tesla stock data are quantitative.  
All `r ncol(googSelected)` selected variables in Google stock data are quantitative.

Before making any changes, let's take a look on the data structures. 
```{r dataStructure}
#Tsla
#str(tslaSelected)
sapply(tsla,function(x){length(unique(x))})
#summary(tsla)

#Goog
#str(googSelected)
sapply(goog,function(x){length(unique(x))})
#summary(tslaSelected)
```

All `r ncol(tslaSelected)` selected variables in Tesla stock data are quantitative.  
All `r ncol(googSelected)` selected variables in Google stock data are quantitative.


```{r highCorr}
#Recall highly correlated variables before making changes

highCorrTsla = tslaSelected %>%
  cor %>%
  caret::findCorrelation(.8, names = TRUE)
highCorrTsla
corrplot::corrplot(cor(tslaSelected), order="hclust")

highCorrGoog = googSelected %>%
  cor %>%
  caret::findCorrelation(.8, names = TRUE)
highCorrGoog
corrplot::corrplot(cor(googSelected), order="hclust")
```

Highly correlated features in Tesla data set are : `r highCorrTsla`.
Highly correlated features in Google data set are : `r highCorrGoog`.

Instead of removing all these features, try to calculate new quantities based on them and check the correlations again with new features added. 

##### Pivot Point, Resistance Levels, Support Levels 
The pivot point itself is simply the average of the high, low and closing prices from the previous trading day.
On the subsequent day, trading above the pivot point is thought to indicate ongoing bullish sentiment, while trading below the pivot point indicates bearish sentiment.

###### Calculation
* Resistance Level 3 = Previous Day High + 2(Pivot – Previous Day Low)
* Resistance Level 2 = Pivot + (Resistance Level 1 – Support Level 1)
* Resistance Level 1 = (Pivot x 2) – Previous Day Low
* Pivot = Previous Day (High + Low + Close) / 3
* Support Level 1 = (Pivot x 2) – Previous Day High
* Support Level 2 = Pivot – (Resistance Level 1 – Support Level 1)
* Support Level 3 = Previous Day Low – 2(Previous Day High – Pivot)

```{r newVar}
###TSLA
tslaTemp = tslaSelected %>% 
  dplyr::mutate(
         PP = lag((PX_HIGH + PX_LOW + PX_LAST)/3), #Previous Day (High + Low + Close) / 3
         R1 = 2 * PP - lag(PX_LOW),                #(Pivot x 2) – Previous Day Low
         S1 = 2 * PP - lag(PX_HIGH),               #(Pivot x 2) – Previous Day High
         R2 = PP + (R1 - S1),                      #Pivot + (R1 – S1)
         S2 = PP - (R1 - S1),                      #Pivot - (R1 – S1)
         R3 = lag(PX_HIGH) + 2 * (PP - lag(PX_LOW)),    
         #R3= Previous Day High + 2(PP – Previous Day Low)
         S3 = lag(PX_LOW) - 2 * (lag(PX_HIGH) - PP),
         #S3= Previous Day Low – 2(Previous Day High – Pivot)
         diffHighLow  = PX_HIGH - PX_LOW,
         diffLastOpen = PX_LAST - PX_OPEN,
         stockReturn  = (PX_LAST - PX_OPEN)/PX_OPEN
         )

highCorrF = function(dta, thred){
  #Find highly correlated features
  highCorr = dta %>%
  cor %>%
  caret::findCorrelation(thred, names = TRUE)
  return(highCorr)
}


hclustPlotF = function(dta) {
  #Correlation Plot
  corrplot::corrplot(cor(dta), order="hclust")
}

###GOOG


```

On Balance Volume is calculated by adding the day's volume to a cumulative total when the security's price closes up, and subtracting the day's volume when the security's price closes down.

If today's close is greater than yesterday's close then:
OBV = Yesterday’s OBV + Today’s Volume

If today’s close is less than yesterday’s close then:
OBV = Yesterday’s OBV – Today’s Volume

If today’s close is equal to yesterday’s close then:
OBV = Yesterday’s OBV

```{r OnBalanceVolume}
tslaAddOBV = tslaTemp %>% 
  mutate(obv = TTR::OBV(PX_LAST, PX_VOLUME))
```

#####Chaikin money flow
Description:
Chaikin Money Flow (CMF) developed by Marc Chaikin is a volume-weighted average of accumulation and distribution over a specified period. The standard CMF period is 21 days. The principle behind the Chaikin Money Flow is the nearer the closing price is to the high, the more accumulation has taken place. Conversely, the nearer the closing price is to the low, the more distribution has taken place. If the price action consistently closes above the bar's midpoint on increasing volume, the Chaikin Money Flow will be positive. Conversely, if the price action consistently closes below the bar's midpoint on increasing volume, the Chaikin Money Flow will be a negative value.

How this indicator works:
* A CMF value above the zero line is a sign of strength in the market, and a value below the zero line is a sign of weakness in the market.
* Wait for the CMF to confirm the breakout direction of price action through trend lines or through support and resistance lines. For example, if a price breaks upward through resistance, wait for the CMF to have a positive value to confirm the breakout direction.
* A CMF sell signal occurs when price action develops a higher high into overbought zones, with the CMF diverging with a lower high and beginning to fall.
* A CMF buy signal occurs when price action develops a lower low into oversold zones, with the CMF diverging with a higher low and beginning to rise.

Calculation:
CMF            = n-day Sum of [(((C - L) - (H - C)) / (H - L)) x Vol] / n-day Sum of Vol
cmfNumerator   = n-day Sum of [(((C - L) - (H - C)) / (H - L)) x Vol]
cmfDenominator = n-day Sum of Vol

Where:
n = number of periods, typically 21
H = high
L = low
C = close
Vol = volume

```{r chaikinMoneyFlow}
#TSLA
daysCMF = 21
tslaAddCMF = tslaAddOBV %>%
  mutate(cmf = TTR::CMF(tslaAddOBV[,c("PX_HIGH","PX_LOW","PX_LAST")], PX_VOLUME, n = daysCMF))
#GOOG
```

#####Exponential Moving Average (EMA)

An exponential moving average (EMA) is a type of moving average (MA) that places a greater weight and significance on the most recent data points. The exponential moving average is also referred to as the exponentially weighted moving average. An exponentially weighted moving average reacts more significantly to recent price changes than a simple moving average (SMA), which applies an equal weight to all observations in the period.

* EMA = Closing price x multiplier + EMA (previous day) x (1-multiplier)

######Limitations of the EMA
It is unclear whether or not more emphasis should be placed on the most recent days in the time period. Many traders believe that new data better reflects the current trend of the security. At the same time, others feel that overweighting recent dates creates a bias that leads to more false alarms.

Similarly, the EMA relies wholly on historical data. Many economists believe that markets are efficient, which means that current market prices already reflect all available information. If markets are indeed efficient, using historical data should tell us nothing about the future direction of asset prices.

EMA calculates an exponentially-weighted mean, giving more weight to recent observations. See
Warning section below.
```{r exponentialMovingAverage}
#TSLA
daysEMA = 10 #EMA lengths
tslaAddEMA = tslaAddCMF %>%
  mutate(emaPrice = TTR::EMA(tslaAddCMF$PX_LAST,daysEMA)) %>%
  mutate(emaVolume = TTR::EMA(tslaAddCMF$PX_VOLUME,daysEMA))

###GOOG
```

```{r relativeStrengthIndex(RSI)}
###Tsla
daysRSI = 14
tslaAddRSI = tslaAddEMA %>%
  mutate(rsi = TTR::RSI(PX_LAST, n=daysRSI, maType="WMA", wts=PX_VOLUME))
###GOOG
```

SMA calculates the arithmetic mean of the series over the past n observations.
```{r simpleMovingAverage(SMA)}
###Tsla

daysSMA = 10
tslaAddSMA = tslaAddRSI %>%
  mutate(smaPrice = TTR::SMA(PX_LAST, n=daysSMA)) %>%
  mutate(smaVolume = TTR::SMA(PX_VOLUME, n=daysSMA))
###GOOG
```

```{r momentumStochasticIndex(K% and D%)}
###Tsla
#Stochastic Momentum Index
daysK = 5 #%K is usually set to 5 and represents the main movements of price – slow line.
daysD = 3 #%D is the fast line, a simple moving average of the %K and is set to 3.
tslaHLC  = tslaSelected %>% dplyr::select(PX_HIGH, PX_LOW, PX_LAST)
tslaAddK = tslaAddSMA %>%
 dplyr::mutate(smi_K = (stoch(tslaHLC, nFastK = daysK, nFastD = daysD, nSlowD = 3, bounded = TRUE,smooth = 1))[,1]) %>% #calculate fastK%
 dplyr::mutate(smi_D = (stoch(tslaHLC, nFastK = daysK, nFastD = daysD, nSlowD = 3, bounded = TRUE,smooth = 1))[,2])  #calculate fastD%

#bounded:Logical, should current period’s values be used in the calculation? Set it as TRUE here. We will align Y's with past X's in Section xxx.
#Reference:https://library.tradingtechnologies.com/trade/chrt-ti-stochastic-momentum-index.html
###GOOG
```

### Moving Average Convergence Divergence
```{r MACD}
daysMACDslow   = 26 #default nSlow
daysMACDfast   = 12 #default nFast
daysMACDsignal = 9  #default nSig
tslaAddMACD = tslaAddK %>%
  mutate(macdPrice = TTR::MACD(PX_LAST, nFast = daysMACDfast, nSlow = daysMACDslow, nSig = daysMACDsignal)) %>%
  #mutate(macdPrice = TTR::MACD(nFast = 12, nSlow = 26, nSig = 9, maType="WMA", wts=PX_VOLUME)) %>% # weighted macdPrice
  mutate(macdVolume = TTR::MACD(PX_VOLUME, nFast = daysMACDfast, nSlow = daysMACDslow, nSig = daysMACDsignal))

```

```{r LarryWilliam's R%}
daysR = 14 #default
#tslaHLC  = tslaSelected %>% dplyr::select(PX_HIGH, PX_LOW, PX_LAST)
tslaAddR = tslaAddMACD %>%
  mutate(larryWilliamsR = TTR::WPR(tslaHLC, n = daysR))
```

TheWilliams Accumulation / Distribution (AD) line is a measure of market momentum. Developed by Larry Williams. The Williams AD line differs from OBV and chaikinAD in that it doesn’t take volume into account.
```{r Accumulation/Distribution}
tslaAddAD = tslaAddR %>%
  mutate(williamsAD = TTR::williamsAD(tslaHLC))

```

The Commodity Channel Index (CCI) attempts to identify starting and ending trends.CCI relates the current price and the average of price over n periods. The CCI usually falls in a channel of -100 to 100. A basic CCI trading system is: Buy (sell) if CCI rises above 100 (falls below -100) and sell (buy) when it falls below 100 (rises above -100).

```{r CommodityChannelIndex(CCI)}
daysCCI = 20
tslaAddCCI = tslaAddAD %>%
  mutate(cci = TTR::CCI(tslaHLC, n = daysCCI))
#--End--#

#Summarize how many indicators we've added. 

beforeAfterVar = tslaAddCCI[,(ncol(tslaSelected)+1):ncol(tslaAddCCI)]
names(beforeAfterVar)

```

Up till now, we added xx stock indicators:
`r names(beforeAfterVar)`

#### Probably delete below codes later ##################
#```{r addLaggedVariables}
#Add lagged values of features. We'll fit time series models later. This can be a fairly good alternative for TS models, which saves us from running the stationarity test for each feature. 

addLaggedVarF = function(feature, dataForModification, featureName,lagSize ) {
  for (lags in 1:lagSize) { 
    newColName = paste0(featureName, lags)
    dataForModification = dataForModification %>%
      dplyr::mutate(!!sym(newColName) := dplyr::lag(feature, n = lags))
  }
  return(dataForModification)
}

#Create lagged features for model fitting (use as an alternative to Time Series Model)
#List all predictors and decide of which predictors we should calculate lagged values.
names(tslaAddCCI)

lags10Days = 10
lags21Days = 21
tslaLagged = addLaggedVarF(tslaAddCCI$PX_HIGH, tslalaggedAddRSI, "PX_HIGH",lags10Days)
for (j in 2:ncol(tslalaggedAddRSI)) {
  tslaLagged = addLaggedVarF(tslalaggedAddRSI[,j], tslaLagged, names(tslalaggedAddRSI)[j],lags10Days)
}


addDiffVarF = function(dataForModification,nColumn) {
  #"tslaSelected" is created to remove features with too many missing values from the original data
  #Purpose: Take the first difference of features in "tslaSelected"
  diffFeature = as_tibble(matrix(rep(0,((nrow(dataForModification)-1)*nColumn)),ncol = nColumn))
  featureName = names(dataForModification)[1:nColumn]
  
  for (j in 1:nColumn) {
    diffFeature[,j] = as_tibble(matrix(diff(dataForModification[,j]), ncol = 1))
    names(diffFeature)[j] = paste0(featureName[j],"_diffPreviousDay")
  }
 diffFeature = lag(diffFeature) #Use the difference between today and yesterday to predict tomorrow; first two rows should be NAs. It will be taken care later with na.omit()
 dataForModification_ = dataForModification[2:nrow(dataForModification),]
 laggedDiff = cbind(dataForModification_,diffFeature)
 laggedDiff %>% transmute()
 return(laggedDiff)
}

tslaAddDiff =  addDiffVarF(tslaSelected,ncol(tslaSelected))
#``` #A few added indicators represent moving average of price or volume in a past designated period. Hence, we probably don't need to include lagged values. 
#### Probably delete above codes later #######################

Up till now, we added xx stock indicators:
`r names(beforeAfterVar)`



```{r assignYandX}
tslaY  = tslaAddCCI %>%
  na.omit() %>%
  dplyr::select(diffHighLow, stockReturn) %>%
  dplyr::as_tibble() %>%
  dplyr::slice(-1)


tslaX    = tslaAddCCI %>%
  na.omit() %>%
  .[1:(nrow(tslaAddCCIRemoveNA)-1),] %>%
  dplyr::as_tibble(.) 

names(tslaY)[1] = "diffHighLowTomorrow" #Y1
names(tslaY)[2] = "stockReturnTomorrow" #Y2

tslaFull = as_tibble(cbind(tslaY,tslaX))

hclustPlotF(tslaFull)

#Correlation between stock data of today and difference between highest and lowest stock prices on tomorrow is much higher than correlation between stock data of today and daily stock return on tomorrow. Thus, it's suggested to use "diffHighLowTomorrow " as the supervisor in model fitting.

summary(tslaY$diffHighLowTomorrow)

```

```{r PCA Regression}
#It's suggested to use only quantitative variables in PCA regression

#Transform the selected data

(tslaXTrans = caret::preProcess(as.data.frame(tslaX), method = c("BoxCox","center","scale","pca")))


#Apply the transformations("BoxCox","center","scale","pca")
pcaTslaX = stats::predict(tslaXTrans,as.data.frame(tslaX)) #Outputs are PCA components
#head(pcaTslaX)

#Summary of PCA results
tslaXTrans_     = caret::preProcess(as.data.frame(tslaX), method = c("BoxCox","center","scale"))
propVar_pcaTslaX = as.data.frame(tslaX) %>%
                 stats::predict(tslaXTrans_,.) %>%
                 stats::prcomp() %>%
                 summary()
propVar_pcaTslaX #proportion of variance explained by each PCs


#Check extreme observations via PCA

ggplot(data = pcaTslaX)+
  geom_point(aes(x = PC1, y= PC2))

ggplot(data = pcaTslaX)+
  geom_point(aes(x = PC3, y= PC2))

ggplot(data = pcaTslaX)+
  geom_point(aes(x = PC1, y= PC3))

plot3D::scatter3D(pcaTslaX$PC1, pcaTslaX$PC2, pcaTslaX$PC3, pch = 16,  
                  theta = 20, phi = 20, main = "PCA - Tesla", xlab = "PC1", 
                  ylab ="PC2", zlab = "PC3", bty = "g", ticktype = "detailed")

#diffHighLow vs PCs
ggplot(data = pcaTslaX)+
  geom_point(aes(x = PC1, y=tslaY$diffHighLowTomorrow), color=ifelse(tslaY$diffHighLowTomorrow > mean(tslaY$diffHighLowTomorrow),"red", "blue"))

ggplot(data = pcaTslaX)+
  geom_point(aes(x = PC1, y=tslaY$diffHighLowTomorrow)
, color=ifelse(tslaX$diffHighLow > mean(tslaX$diffHighLow),"red", "blue"))

ggplot(data = pcaTslaX)+
  geom_point(aes(x = PC1, y=tslaY$diffHighLowTomorrow)
, color=ifelse(tslaX$stockReturn > mean(tslaX$stockReturn),"red", "blue"))

ggplot(data = pcaTslaX)+
  geom_point(aes(x = PC2, y=tslaY$diffHighLowTomorrow), color=ifelse(tslaY$diffHighLowTomorrow > mean(tslaY$diffHighLowTomorrow),"red", "blue"))

ggplot(data = pcaTslaX)+
  geom_point(aes(x = PC2, y=tslaY$diffHighLowTomorrow)
, color=ifelse(tslaX$diffHighLow > mean(tslaX$diffHighLow),"red", "blue"))

ggplot(data = pcaTslaX)+
  geom_point(aes(x = PC2, y=tslaY$diffHighLowTomorrow)
, color=ifelse(tslaX$stockReturn > mean(tslaX$stockReturn),"red", "blue"))

ggplot(data = pcaTslaX)+
  geom_point(aes(x = PC3, y=tslaY$diffHighLowTomorrow)
             , color=ifelse(tslaY$diffHighLowTomorrow > mean(tslaY$diffHighLowTomorrow),"red", "blue"))

ggplot(data = pcaTslaX)+
  geom_point(aes(x = PC3, y=tslaY$diffHighLowTomorrow)
, color=ifelse(tslaX$diffHighLow > mean(tslaX$diffHighLow),"red", "blue"))

ggplot(data = pcaTslaX)+
  geom_point(aes(x = PC3, y=tslaY$diffHighLowTomorrow)
, color=ifelse(tslaX$stockReturn > mean(tslaX$stockReturn),"red", "blue"))

scatterplot3d::scatterplot3d(x = pcaTslaX$PC1, y = pcaTslaX$PC2,
                             z = tslaY$diffHighLowTomorrow, pch = 16, color = ifelse(tslaY$diffHighLowTomorrow > mean(tslaY$diffHighLowTomorrow),"red", "blue"))

#stockReturn vs PCs

ggplot(data = pcaTslaX)+
  geom_point(aes(x = PC1, y=tslaY$stockReturnTomorrow), color=ifelse(tslaX$stockReturn > mean(tslaX$stockReturn),"red", "blue"))

ggplot(data = pcaTslaX)+
  geom_point(aes(x = PC1, y=tslaY$stockReturnTomorrow), color=ifelse(tslaX$diffHighLow > mean(tslaX$diffHighLow),"red", "blue")) 

ggplot(data = pcaTslaX)+
  geom_point(aes(x = PC1, y=tslaY$stockReturnTomorrow), color=ifelse(tslaX$stockReturn > mean(tslaX$stockReturn),"red", "blue"))

ggplot(data = pcaTslaX)+
  geom_point(aes(x = PC2, y=tslaY$stockReturnTomorrow))

ggplot(data = pcaTslaX)+
  geom_point(aes(x = PC3, y=tslaY$stockReturnTomorrow))


```


```{r dummyVarForLogisticRegression}
#hist(tslaReg$stockReturn, breaks=20, freq=FALSE,main = paste("Density Plot - ",names(tslaReg$stockReturn)),xlab = " ") # stock return is normally distributed.

XtslaAddLevels = tslaFull %>%
  mutate(aboveR1 = factor(ifelse(PX_HIGH > R1, "Yes", "No"))) %>%
  mutate(aboveR2 = factor(ifelse(PX_HIGH > R2, "Yes", "No"))) %>%
  mutate(aboveR3 = factor(ifelse(PX_HIGH > R3, "Yes", "No"))) %>%
  mutate(belowS1 = factor(ifelse(PX_LOW  < S1, "Yes", "No"))) %>%
  mutate(belowS2 = factor(ifelse(PX_LOW  < S2, "Yes", "No"))) %>%
  mutate(belowS3 = factor(ifelse(PX_LOW  < S3, "Yes", "No"))) %>%
  select(aboveR1, aboveR2, aboveR3, belowS1, belowS2, belowS3)            
XtslaDummyModel = caret::dummyVars(~ ., data = XtslaAddLevels, fullRank = TRUE)

YtslaAddLevels = as_tibble(factor(ifelse(tslaY$stockReturnTomorrow > 0, "Increased", "Decreased"))) 

YtslaDummyModel = caret::dummyVars(~ ., data = YtslaAddLevels, fullRank = TRUE)

XtslaDummy = as_tibble(predict(XtslaDummyModel, XtslaAddLevels))
YtslaDummy = as_tibble(predict(YtslaDummyModel, YtslaAddLevels))

names(YtslaDummy)[1] = "stockReturnTomorrowDummy" #Y2

tslaLogReg = tslaFull %>%
  mutate(XtslaDummy) %>% 
  dplyr::select(-R1,-S1,-R2,-S2,-R3,-S3) %>% 
  #Drop PP, R1, R2, R3, S1, S2, S3 
  mutate(YtslaDummy) 

```

#```{r removeHighCorr}
hclustPlotF(tslaLogReg)
XtslaLogReg = tslaLogReg %>%
  select(-diffHighLowTomorrow, -stockReturnTomorrow,-stockReturnTomorrowDummy)

highCorrTsla = highCorrF(tslaFull,0.8)
highCorrTsla

tslaLogRegRemoveCorr = XtslaLogReg %>% select(-highCorrTsla)
hclustPlotF(tslaLogRegRemoveCorr)
#```

```{r linearRegression}
#First, correct skewness
tslaXyeoJ = tslaX %>%
  caret::preProcess(method = "YeoJohnson") %>%
  predict(newdata = tslaX)

#hist(tslaY$diffHighLowTomorrow)
#hist(tslaY$stockReturnTomorrow)
tslaYyeoJ = tslaY %>%
  caret::preProcess(method = "YeoJohnson") %>%
  predict(newdata = tslaY)
#hist(tslaYyeoJ$diffHighLowTomorrow)
#hist(tslaYyeoJ$stockReturnTomorrow)

Ktsla = 10
cvIndexTsla = caret::createFolds(tslaYyeoJ$diffHighLowTomorrow, k = Ktsla, returnTrain = TRUE)
trControl = trainControl(method = "cv", index = cvIndexTsla)

XtslaPenalty = tslaXyeoJ %>%
  caret::preProcess(method = c("center","scale")) %>%
  predict(tslaXyeoJ) %>%
  cbind(.,XtslaDummy) %>%
  dplyr::select(-R1,-S1,-R2,-S2,-R3,-S3)

YtslaPenalty = tslaYyeoJ %>%
  caret::preProcess(method = c("center","scale")) %>%
  predict(newdata = tslaYyeoJ)

lmOutTsla = train(x = XtslaPenalty, y = YtslaPenalty$diffHighLowTomorrow, method = "lm", trControl = trControl)

lmOutTsla
```

```{r penalizedRegression}
#Errors in ridge and lasso regression are solved by removing highly correlated features in XtslaPenalty
highCorrTsla = XtslaPenalty %>% highCorrF(.,0.8) %>%
              .[-c(25,27)] %>%
  c(.,c("macdPrice","macdVolume")) 
XtslaRegRemoveCorr = XtslaPenalty %>% select(-highCorrTsla)

names(XtslaPenalty)
names(XtslaLogRegRemoveCorr)

#ridge regression
ridgeOutTsla = train(x = XtslaRegRemoveCorr, y = YtslaPenalty$diffHighLowTomorrow, method = "ridge", tuneLength = 10,trControl = trControl)

ridgeOutTsla

ridgeOutTsla = train(x = XtslaPenalty, y = YtslaPenalty$diffHighLowTomorrow, method = "ridge", tuneLength = 10,trControl = trControl)

#lasso regression
lassoOutTsla = train(x = XtslaRegRemoveCorr, y = YtslaPenalty$diffHighLowTomorrow, method = "lasso", tuneLength = 10,trControl = trControl)

lassoOutTsla

#elastic net
gridLambdaAlpha = expand.grid(lambda = seq(1e-6, 0.05, length = 15), alpha = c(0,0.25,.5,.75,1))

enetOutTsla = train(x = XtslaPenalty, y = YtslaPenalty$diffHighLowTomorrow,
                method = "glmnet", tuneGrid = gridLambdaAlpha, trControl = trControl)

enetOutTsla

plot(enetOutTsla, xlab = "Lambda (Elastic Net Penalty)", ylab = "RMSE (K-fold CV)" )

#Results of penalized linear model
names(enetOutTsla$finalModel)

alphaHatTsla  = enetOutTsla$finalModel$tuneValue$alpha
lambdaHatTsla = enetOutTsla$finalModel$tuneValue$lambda

```

>Tesla (TSLA)

The CV - minimizing values are alpha = `r alphaHatTsla` and lambda = `r lambdaHatTsla`.
Next, refit the model with this alpha by calling "glmnet" directly and select the lambdaHat that minimized CV. 

Use results from  Elastic Net Regression to find coefficients Beta's.

```{r betaPenalizedRegression}
glmnetOutTsla = glmnet::glmnet(x = as.matrix(XtslaPenalty), y = YtslaPenalty$diffHighLowTomorrow, alpha = alphaHatTsla, standardize = FALSE)
betaHatTsla = coef(glmnetOutTsla, s = lambdaHatTsla)
betaHatTsla
##Question: Collinearity in Elastic Net regression
```

Many features in our data are not normally distributed, which violates the normality assumption for LDA. Thus, we'll fit our data with a logistic regression model. 

#```{r logisticRegression}

trControl = trainControl(method = 'none')
tslaOutLogistic = train(x = XtslaLogReg, y = YtslaDummy, method = 'glm', trControl = trControl)

summary(tslaOutLogistic)


#Forward Selection
tslaOutForward = leaps::regsubsets(x =tslaLogRegRemoveCorr, y = factor(YtslaDummy$value.Increased), nvmax=ncol(tslaLogRegRemoveCorr), method='forward')
tslaSumForward      = summary(tslaOutForward)
tslamodelForward    = tslaSumForward$which[which.min(tslaSumForward$bic),]

(tslaSelectedForward = tslamodelForward[-1])

#####################################################################################
#Use selected features from Forward Selection to fit another logistic model
XtslaForward = as_tibble(select(XtslaFull, diffHighLow2  , diffLastOpen1,ASK_diff, cmf10))

tslaOutLogistic2 = caret::train(x = XtslaForward, y = YtslaFull, method = 'lm', trControl = trControl)
summary(tslaOutLogistic2)

##alternative:use glm() and library(mass)
library(MASS)
XtslaLogReg = tslaLogReg %>%
  select(-diffHighLowTomorrow, -stockReturnTomorrow,-stockReturnTomorrowDummy)

tslaOutLogistic = glm(stockReturnTomorrowDummy ~ ., family = binomial, data = tslaLogReg)

summary(tslaOutLogistic)
library(bestglm)
tslaBestGLM = 
tslaBestGlmFit = bestglm::bestglm(dataframeWithYatLast, family = binomial, IC="BIC")

tlsaLinearRegFull = tslaFull %>%
  select(-stockReturnTomorrow)


#```

The selected logistic model has very low R-squared but all coefficients 

```{r dataSplit}
#training set
#validation set
#test set
```
```{r logisticRegressionAfterDataSplit}
#Select tuning parameters with validation set
#Fit logistic models with training set
```

```{r modelPerformance}
#Calculate test errors

#confusion matrix

```

##### 6 - Neural Networks


##### 7 - SVM


##### 8 - Random Forest