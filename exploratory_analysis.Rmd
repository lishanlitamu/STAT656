---
title: "Exploratory Analysis"
author: "Kyle Dixon, Mia Li, Caitlin Hennessey"
date: "9/4/2020"
output: html_document
---

## Project Topic:
## Analyzing the impact of news and social media sentiment on stock prices.

### 1 - General review of raw data
#### 1.1 Load and clean data
```{r initialize, include = F}
## Load libraries
library(bit64) # To read raw data
library(caret)
library(corrplot)
library(data.table) # Faster CSV reader than base
library(doParallel)
library(dplyr)
library(e1071)
library(ggplot2) # General plotting
library(GuardianR) # Guardian API scrape
library(newsanchor) # News API scrape
library(parallel)
library(readxl)
library(rtweet) # Twitter scrape
library(rvest)
library(stringr) # Goggle news scrape
library(tau)
library(textdata)
library(plot3D)   #PCs plot
#library(RcppRoll) #CMF
library(TTR)      #Calculate Indicators: EMA,SMA,RSI
library(leaps)    #Stepwise Model Selection
library(zoo)      # Interpolation
library(elasticnet) #ridge, lasso, elastic net regression
library(earth)    #MARS, multivariate adaptive regression splines
library(vip)      #Variable importance, lecture 27
library(glmnet)   #Elastic net regression
#library(keras)    #Neural Networks
#library(tensorflow)
library(foreach)

knitr::opts_chunk$set(echo = T)
```

```{r clean_data}
## Load raw data into the global environment
tsla <- base::data.frame(data.table::fread("1 - Data/Tesla/tsla.csv", na.strings = c("#N/A N/A", "#N/A Invalid Field", "#N/A Requesting Data...")), stringsAsFactors = F)
goog <- base::data.frame(data.table::fread("1 - Data/Google/goog.csv", na.strings = c("#N/A N/A", "#N/A Invalid Field", "#N/A Requesting Data...")), stringsAsFactors = F)

## Clean raw data
tsla <- tsla %>% dplyr::select_if(~sum(!is.na(.)) > 0) %>% # Drop columns that contain only NAs
  dplyr::filter(!is.na(PX_LAST)) # Drop rows where PX_LAST is NA
goog <- goog %>% dplyr::select_if(~sum(!is.na(.)) > 0) %>% # Drop columns that contain only NAs
  dplyr::filter(!is.na(PX_LAST)) # Drop rows where PX_LAST is NA

## Format date columns
tsla$Date <- as.Date(tsla$Date, "%m/%d/%Y")
goog$Date <- as.Date(goog$Date, "%m/%d/%Y")
```

#### 1.2 Visualize data

```{r plot_data}
## Plot prices versus time
tsla_plot <- ggplot2::ggplot() +
  ggplot2::geom_line(data = tsla, ggplot2::aes(x = Date, y = PX_LAST)) + ggplot2::ggtitle("TSLA")
goog_plot <- ggplot2::ggplot() +
  ggplot2::geom_line(data = goog, ggplot2::aes(x = Date, y = PX_LAST)) + ggplot2::ggtitle("GOOG") +
  ggplot2::scale_x_date(limits = c(base::min(tsla$Date), base::max(tsla$Date))) # Align the x-axis with TSLA
gridExtra::grid.arrange(tsla_plot, goog_plot)
```

### 2 - Data Preprocessing (Missingness and Collinearity)
#### 2.1 Data Interpolation

As we have both daily and quarterly updated features aligned in the original data set, quarterly updated features contains mainly missing values. Instead of deleting those features, missing values are interpolated between quarters.   

``` {r interpolation}
## TSLA
## Run linear interpolations
tmp <- zoo::na.approx(tsla[, base::c(14:22, 24:26)])

## Create empty matrix as placeholder for non-interpolated rows
row_diff <- base::nrow(tsla) - base::nrow(tmp)
empty <- base::matrix(base::rep(NA, row_diff * base::ncol(tmp)), nrow = row_diff, ncol = base::ncol(tmp))
base::colnames(empty) <- base::colnames(tmp)  
tmp <- base::rbind(tmp, empty)

## Replace original data with interpolation
tsla[, base::c(14:22, 24:26)] <- tmp

## GOOG
## Run linear interpolations
tmp <- zoo::na.approx(goog[, 11:23])

## Create empty matrix as placeholder for non-interpolated rows
row_diff <- base::nrow(goog) - base::nrow(tmp)
empty <- base::matrix(base::rep(NA, row_diff * base::ncol(tmp)), nrow = row_diff, ncol = base::ncol(tmp))
base::colnames(empty) <- base::colnames(tmp)  
tmp <- base::rbind(tmp, empty)

## Replace original data with interpolation
goog[, 11:23] <- tmp
```

#### 2.2 Count the number of Missing Values in each feature

```{r numNA, warning=FALSE}
require(dplyr)
totalRowTsla = base::nrow(tsla)
totalColTsla = base::ncol(tsla)

naTsla       = tsla %>%
               base::sapply(.,function(y) base::sum(base::length(base::which(base::is.na(y))))) %>%
               base::as.data.frame() %>%
               tibble::rownames_to_column(., "Features")


totalRowGoog = base::nrow(goog)
totalColGoog = base::ncol(goog)

naGoog       = goog %>%
               base::sapply(.,function(y) base::sum(base::length(base::which(base::is.na(y))))) %>%
               base::as.data.frame() %>%
               tibble::rownames_to_column(., "Features")
```

>__Tesla__

Total number of rows: `r totalRowTsla`

Total number of columns: `r totalColTsla`


```{r naTslaKnit, echo=FALSE}
knitr::kable(
 naTsla,
 col.names = c("Features","Number of Missing Values"),
 caption   = "Table 2-1: The number of Missing Values - Tesla",
 align     = "lccrr"
)
```

>__Google__

Total number of rows: `r totalRowGoog`

Total number of columns: `r totalColGoog`


```{r naGoogKnit, echo=FALSE}
knitr::kable(
 naGoog,
 col.names = c("Features","Number of Missing Values"),
 caption   = "Table 2-2: The number of Missing Values - Google",
 align     = "lccrr"
)
```


#### 2.3 Data Transformation

##### 2.3.1 Data Filtering - Tesla
Features with less than 100 missing values will be included by deleting corresponding rows with missing values instead.
```{r filterTsla, warning=FALSE}
#Select a subset of original data for data transformation and PCA
tslaSelected = tsla %>%
  .[,(which(naTsla[,2] < 100))] %>%
  .[!(apply(.,1, function(y){any(is.na(y))})),] %>%
  .[,-1] #remove dates in the first column

#Apply Log-transformation on tslaSelected$TURNOVER to avoid producing NaN in skewness computation
tslaSelected$TURNOVER = log(tslaSelected$TURNOVER)
base::names(tslaSelected) [6]= "log(TURNOVER)"
```

##### 2.3.2 Calculate Skewness and Kurtosis - Tesla
```{r skewnessTsla, warning=FALSE}
require(e1071)
skewnessTsla = apply(tslaSelected,2,e1071::skewness)
kurtosisTsla   = apply(tslaSelected,2,e1071::kurtosis)
```

```{r skewnessTslaTable, warning=FALSE, echo=FALSE}
skewnessTsla = skewnessTsla %>%
               as.data.frame() %>%
               tibble::rownames_to_column(., "features")

knitr::kable(
 skewnessTsla,
 col.names = c("Features","Skewness"),
 caption   = "Table 2-3: Skewness of Selected Features - Tesla",
 align     = "lccrr"
)

kurtosisTsla = kurtosisTsla %>%
               as.data.frame() %>%
               tibble::rownames_to_column(., "features")

knitr::kable(
 kurtosisTsla,
 col.names = c("Features","Kurtosis"),
 caption   = "Table 2-4: Kurtosis of Selected Features - Tesla",
 align     = "lccrr"
)
```

##### 2.3.3 Data Transformation - Tesla
```{r dataTransTsla, warning=FALSE}
#Transform the selected data
(tslaTrans = caret::preProcess(tslaSelected, method = c("BoxCox","center","scale","pca")))

#Apply the transformations("BoxCox","center","scale","pca")
tslaPCA = stats::predict(tslaTrans,tslaSelected) #Outputs are PCA components
utils::head(tslaPCA)

#Summary of PCA results
tslaTrans_     = caret::preProcess(tslaSelected, method = c("BoxCox","center","scale"))
propVarTslaPCA = tslaSelected %>%
                 stats::predict(tslaTrans_,.) %>%
                 stats::prcomp() %>%
                 summary()
propVarTslaPCA #proportion of variance explained by each PCs

#Check extreme observations via PCA

ggplot(data = tslaPCA)+
  geom_point(aes(x = PC1, y= PC2))

ggplot(data = tslaPCA)+
  geom_point(aes(x = PC3, y= PC2))

ggplot(data = tslaPCA)+
  geom_point(aes(x = PC1, y= PC3))


plot3D::scatter3D(tslaPCA$PC1, tslaPCA$PC2, tslaPCA$PC3, pch = 16,  
                  theta = 20, phi = 20, main = "PCA - Tesla", xlab = "PC1", 
                  ylab ="PC2", zlab = "PC3", bty = "g", ticktype = "detailed")

```

#### Fig. 2.1 Density Plots  - Tesla
```{r dataPlotsTsla, echo=FALSE}
transformedTsla = predict(tslaTrans_,tslaSelected)
par(mfrow=c(2,2))
for (i in 1:ncol(tslaSelected)) {
  hist(tslaSelected[,i], breaks=20, freq=FALSE,
       main = paste("Density Plot - ",names(tslaSelected)[i],                            "(Before)"),xlab = " ")
  hist(transformedTsla[,i], breaks=20, freq=FALSE,
       main = paste("Density Plot - ",names(transformedTsla)[i],
                          "(After)"),xlab = " ")
}
```

##### 2.3.4 Data Filtering - Google
Features with less than 100 missing values will be included by deleting corresponding rows with missing values instead.
```{r filterGoog, warning=FALSE}
#Select a subset of original data for data transformation and PCA
googSelected = goog %>%
  .[,(which(naGoog[,2] < 100))] %>%
  .[!(apply(.,1, function(y){any(is.na(y))})),] %>%
  .[,-1] #remove dates in the first column

#Apply Log-transformation on googSelected$TURNOVER
googSelected$TURNOVER = base::log(googSelected$TURNOVER)
base::names(googSelected) [6]= "log(TURNOVER)"
```

##### 2.3.5 Calculate Skewness and Kurtosis - Google
```{r skewnessGoog, warning=FALSE}
#require(e1071)
skewnessGoog = apply(googSelected,2,e1071::skewness)
kurtosisGoog = apply(googSelected,2,e1071::kurtosis)
```

```{r skewnessGoogTable, warning=FALSE, echo=FALSE}
skewnessGoog = skewnessGoog%>%
               as.data.frame() %>%
               tibble::rownames_to_column(., "features")

knitr::kable(
 skewnessGoog,
 col.names = c("Features","Skewness"),
 caption   = "Table 2-5: Skewness of Selected Features - Google",
 align     = "lccrr"
)


kurtosisGoog = kurtosisGoog %>%
               as.data.frame() %>%
               tibble::rownames_to_column(., "features")

knitr::kable(
 kurtosisGoog,
 col.names = c("Features","Kurtosis"),
 caption   = "Table 2-6: Kurtosis of Selected Features - Google",
 align     = "lccrr"
)

```

##### 2.3.6 Data Transformation - Google
```{r dataTransGoog, warning=FALSE}
#Transform the selected data
(googTrans = caret::preProcess(googSelected, method = c("BoxCox","center","scale","pca")))

#Apply the transformations("BoxCox","center","scale","pca")
googPCA = stats::predict(googTrans,googSelected) #Outputs are PCA components
utils::head(googPCA)

#Summary of PCA results
(googTrans_    = caret::preProcess(googSelected, method = c("BoxCox","center","scale")))
propVarGoogPCA = googSelected %>%
                 stats::predict(googTrans_,.) %>%
                 stats::prcomp() %>%
                 summary()
propVarGoogPCA #proportion of variance explained by each PCs

#Check extreme observations via PCA
ggplot(data = googPCA)+
  geom_point(aes(x = PC1, y= PC2))

ggplot(data = googPCA)+
  geom_point(aes(x = PC3, y= PC2))

ggplot(data = googPCA)+
  geom_point(aes(x = PC1, y= PC3))

plot3D::scatter3D(googPCA$PC1, googPCA$PC2, googPCA$PC3, pch = 16,  
                  theta = 20, phi = 20, main = "PCA - Alphabet", xlab = "PC1", 
                  ylab ="PC2", zlab = "PC3", bty = "g", ticktype = "detailed")

```

#### Fig. 2.2 Density Plots  - Google
```{r dataPlotsGoog, echo=FALSE}
transGoog = predict(googTrans_,googSelected)
par(mfrow=c(2,2))
for (i in 1:ncol(googSelected)) {
  hist(googSelected[,i], breaks=20, freq=FALSE,
       main = paste("Density Plot - ",names(googSelected)[i], "(After)"),
  xlab = " ")
  hist(transGoog[,i], breaks=20, freq=FALSE,
       main = paste("Density Plot - ",names(transGoog)[i], "(Before)"),
  xlab = " ")
}
```

#### 2.4 Correlation Analysis
>__Tesla__

```{r findCorrelationTsla, warning=FALSE}
library(corrplot)
corrTsla     = stats::cor(tslaSelected)
corrplot::corrplot(corrTsla, order="hclust")
```

#### Fig. 2.3 Correlations between features - Tesla

>__Google__

```{r findCorrelationGoog, warning=FALSE}
#library(corrplot)
corrGoog     = stats::cor(googSelected)
corrplot::corrplot(corrGoog, order="hclust")
```

#### Fig. 2.4 Correlations between features - Google

### 3 - News Scraping

```{r nees_scrape}
## Guardian API key
api_key <- "d02efa1b-b4a4-4e20-a7f5-eee62d80cbf6"

## Query Guardian for TSLA news
tsla_news <- GuardianR::get_guardian("Tesla", section = base::c("business", "technology", 
                                                                "money", "world"), 
                                     from.date = "2015-10-02", to.date = "2020-09-01", api.key = api_key)
tsla_news <- tsla_news[, 4:5] # Extract relevant columns only

## Format date column
tsla_news$webPublicationDate <- base::as.Date(base::substr(tsla_news$webPublicationDate, 1, 10), 
                                              "%Y-%m-%d")

## Query Guardian for GOOG news
goog_news <- GuardianR::get_guardian("Alphabet", section = base::c("business", "technology", 
                                                                   "money", "world"), 
                                     from.date = "2015-10-02", to.date = "2020-09-01", api.key = api_key)
goog_news <- goog_news[, 4:5] # Extract relevant columns only

## Format date column
goog_news$webPublicationDate <- base::as.Date(base::substr(goog_news$webPublicationDate, 1, 10), 
                                              "%Y-%m-%d")

## Get positive and negative words from Loughran and McDonald
positive_words <- base::data.frame(
  readxl::read_excel("2 - Word Lists/LoughranMcDonald_SentimentWordLists_2018.xlsx",
                     sheet = "Positive", col_names = F))$...1

negative_words <- base::data.frame(
  readxl::read_excel("2 - Word Lists/LoughranMcDonald_SentimentWordLists_2018.xlsx",
                     sheet = "Negative", col_names = F))$...1

## Calculate sentiment for TSLA
tsla_news$Sentiment <- NA
for (row in 1:base::nrow(tsla_news)) {
  tmp <- base::strsplit(tsla_news[row, ]$webTitle, "[[:space:]]|(?=[.!?])", perl = T)[[1]]
  
  sentiment <- 0
  for (word in tmp) {
    if (base::nchar(word) > 1 & !(base::grepl("\\(", word)) & !(base::grepl("\\)", word))) {
      if (base::any(stringr::str_detect(positive_words, stringr::regex(word, ignore_case = T)))) {
        sentiment <- sentiment + 1
      } else if (base::any(stringr::str_detect(negative_words, stringr::regex(word, ignore_case = T)))) {
        sentiment <- sentiment - 1
      }
    }
  }
  tsla_news[row, ]$Sentiment <- sentiment
}

## Calculate sentiment for GOOG
goog_news$Sentiment <- NA
for (row in 1:base::nrow(goog_news)) {
  tmp <- base::strsplit(goog_news[row, ]$webTitle, "[[:space:]]|(?=[.!?])", perl = T)[[1]]
  
  sentiment <- 0
  for (word in tmp) {
    if (base::nchar(word) > 1 & !(base::grepl("\\(", word)) & !(base::grepl("\\)", word))) {
      if (base::any(stringr::str_detect(positive_words, stringr::regex(word, ignore_case = T)))) {
        sentiment <- sentiment + 1
      } else if (base::any(stringr::str_detect(negative_words, stringr::regex(word, ignore_case = T)))) {
        sentiment <- sentiment - 1
      }
    }
  }
  goog_news[row, ]$Sentiment <- sentiment
}

goog_news$Ticker <- "GOOG"
tsla_news$Ticker <- "TSLA"
plot_data <- dplyr::bind_rows(goog_news, tsla_news)
mu <- plyr::ddply(plot_data, "Ticker", summarise, grp.mean = mean(Sentiment))
ggplot2::ggplot(data = plot_data, ggplot2::aes(x = Sentiment, fill = Ticker)) + 
  ggplot2::geom_density(alpha = 0.5) + 
  ggplot2::geom_vline(data = mu, ggplot2::aes(xintercept = grp.mean, color = Ticker),
                      linetype = "dashed") +
  ggplot2::scale_color_manual(values = c("#5f506b", "#86bbbd")) + 
  ggplot2::scale_fill_manual(values = c("#5f506b", "#86bbbd"))
```

### 4 - Twitter Scraping

```{r twitter_scrape}
## Twitter API keys and secrets
api_key <- "zElIwld33bNohcb28j7si3aBW"
api_secret_key <- "a5yDRmHeFpq2tpqk4u687rAyDuApzUe2ZffYir3disyqa98QGb"
bearer_token <- "AAAAAAAAAAAAAAAAAAAAAH2iGAEAAAAApwFAjq60a%2Ba8gOG2URX2scAHOo0%3DPXz3xx1Z5i8YuqNzRt6wSAINxOkqNrArIESFlnxWCToPHhEcRH"
access_token <- "1271553006522306568-Azxsa0o7nHOCNpvRjRdocbWPYz0D6w"
access_token_secret <- "PwT1GB0Ogj0poevifeUShWB4lEZAODXHWfm8HyXRz2J7e"

## Create Twitter token
twitter_token <- rtweet::create_token(app = "BNC Sentiment Analysis", consumer_key = api_key, consumer_secret = api_secret_key, access_token = access_token, access_secret = access_token_secret)

## Get positive and negative words from Loughran and McDonald
positive_words <- base::data.frame(
  readxl::read_excel("2 - Word Lists/LoughranMcDonald_SentimentWordLists_2018.xlsx",
                     sheet = "Positive", col_names = F))$...1

negative_words <- base::data.frame(
  readxl::read_excel("2 - Word Lists/LoughranMcDonald_SentimentWordLists_2018.xlsx",
                     sheet = "Negative", col_names = F))

## Loop through tickers and pull tweets
sentiment <- base::data.frame("Ticker" = base::character(),
                              "Date" = base::as.Date(base::character()),
                              "Sentiment" = base::numeric(), stringsAsFactors = F)

cl <- parallel::makeCluster(3, setup_strategy = "sequential")
doParallel::registerDoParallel(cl)
foreach(index = 1:2) %dopar% {
## Twitter API keys and secrets
  library(dplyr)
  tickers <- c("TSLA", "GOOG"); ticker <- tickers[index]
  
  ## Pull tweets (this will throw an authentication page the first time)
  search <- base::ifelse(ticker == "TSLA", "Tesla", "Google")
  tweets <- rtweet::search_tweets(q = search, n = 1000, include_rts = F,
                                  retryonratelimit = T)

  ## Check sentiment
  tweets$sentiment <- base::as.numeric(NA)
  for(row in 1:base::nrow(tweets)) {
    values <- base::numeric()
    for (word in base::strsplit(tweets[row, ]$text, "[[:punct:] ]")[[1]]) {
      if (word %in% positive_words) {values <- append(values, 1)}
      else if (word %in% negative_words) {values <- append(values, -1)}
      else {values <- append(values, 0)}
    }
  tweets[row, ]$sentiment <- base::sum(values)
  }

  tweets$sentiment <- base::round(stats::weighted.mean(tweets$sentiment,
                                                 w = tweets$favorite_count + tweets$favorite_count),
                            digits = 1)
  tweets$created_at <- base::as.Date(tweets$created_at, "%Y-%m-%d")
  tmp <- tweets %>% dplyr::group_by(created_at) %>% dplyr::summarize("sentiment" = base::sum(sentiment))
  sentiment <- base::append(sentiment, tmp)
}
parallel::stopCluster(cl)
foreach::registerDoSEQ()

names(sentiment) <- c("TSLA", "GOOG")

ggplot2::ggplot(sentiment, ggplot2::aes(x = Date, y = Sentiment, color = Ticker)) + ggplot2::geom_line() + ggplot2::ggtitle("Twitter Sentiment Over Time")



```

### 5 - PCA Regression
#### 5.1 Data Preprocessing

Before making changes, let's take a look on the data structures. 
```{r dataStructure}
#Tsla
#str(tslaSelected)
sapply(tslaSelected,function(x){length(unique(x))})

#Goog
#str(googSelected)
sapply(googSelected,function(x){length(unique(x))})
```

All `r ncol(tslaSelected)` selected variables in Tesla stock data are quantitative.  
All `r ncol(googSelected)` selected variables in Google stock data are quantitative.




```{r highCorr}
#Recall highly correlated variables before making changes

#TSLA
highCorrTsla = tslaSelected %>%
  cor %>%
  caret::findCorrelation(.8, names = TRUE)
highCorrTsla
corrplot::corrplot(cor(tslaSelected), order="hclust")
#GOOG
highCorrGoog = googSelected %>%
  cor %>%
  caret::findCorrelation(.8, names = TRUE)
highCorrGoog
corrplot::corrplot(cor(googSelected), order="hclust")
```

Highly correlated features in Tesla data set are : `r highCorrTsla`.
Highly correlated features in Google data set are : `r highCorrGoog`.

Instead of removing all these features, try to calculate new quantities based on them and check the correlations again with new features added. 

##### Pivot Point, Resistance Levels, Support Levels 
The pivot point itself is simply the average of the high, low and closing prices from the previous trading day.
On the subsequent day, trading above the pivot point is thought to indicate ongoing bullish sentiment, while trading below the pivot point indicates bearish sentiment.

###### Calculation
* Resistance Level 3 = Previous Day High + 2(Pivot – Previous Day Low)
* Resistance Level 2 = Pivot + (Resistance Level 1 – Support Level 1)
* Resistance Level 1 = (Pivot x 2) – Previous Day Low
* Pivot = Previous Day (High + Low + Close) / 3
* Support Level 1 = (Pivot x 2) – Previous Day High
* Support Level 2 = Pivot – (Resistance Level 1 – Support Level 1)
* Support Level 3 = Previous Day Low – 2(Previous Day High – Pivot)

```{r newVar}
###TSLA
tslaTemp = tslaSelected %>% 
  dplyr::mutate(
         PP = lag((PX_HIGH + PX_LOW + PX_LAST)/3), #Previous Day (High + Low + Close) / 3
         R1 = 2 * PP - lag(PX_LOW),                #(Pivot x 2) – Previous Day Low
         S1 = 2 * PP - lag(PX_HIGH),               #(Pivot x 2) – Previous Day High
         R2 = PP + (R1 - S1),                      #Pivot + (R1 – S1)
         S2 = PP - (R1 - S1),                      #Pivot - (R1 – S1)
         R3 = lag(PX_HIGH) + 2 * (PP - lag(PX_LOW)),    
         #R3= Previous Day High + 2(PP – Previous Day Low)
         S3 = lag(PX_LOW) - 2 * (lag(PX_HIGH) - PP),
         #S3= Previous Day Low – 2(Previous Day High – Pivot)
         diffHighLow  = PX_HIGH - PX_LOW,
         diffLastOpen = PX_LAST - PX_OPEN,
         stockReturn  = (PX_LAST - PX_OPEN)/PX_OPEN
         )

highCorrF = function(dta, thred){
  #Find highly correlated features
  highCorr = dta %>%
  cor %>%
  caret::findCorrelation(thred, names = TRUE)
  return(highCorr)
}


hclustPlotF = function(dta) {
  #Correlation Plot
  corrplot::corrplot(cor(dta), order="hclust")
}

###GOOG
googTemp = googSelected %>% 
  dplyr::mutate(
         PP = lag((PX_HIGH + PX_LOW + PX_LAST)/3), #Previous Day (High + Low + Close) / 3
         R1 = 2 * PP - lag(PX_LOW),                #(Pivot x 2) – Previous Day Low
         S1 = 2 * PP - lag(PX_HIGH),               #(Pivot x 2) – Previous Day High
         R2 = PP + (R1 - S1),                      #Pivot + (R1 – S1)
         S2 = PP - (R1 - S1),                      #Pivot - (R1 – S1)
         R3 = lag(PX_HIGH) + 2 * (PP - lag(PX_LOW)),    
         #R3= Previous Day High + 2(PP – Previous Day Low)
         S3 = lag(PX_LOW) - 2 * (lag(PX_HIGH) - PP),
         #S3= Previous Day Low – 2(Previous Day High – Pivot)
         diffHighLow  = PX_HIGH - PX_LOW,
         diffLastOpen = PX_LAST - PX_OPEN,
         stockReturn  = (PX_LAST - PX_OPEN)/PX_OPEN
         )
```

On Balance Volume is calculated by adding the day's volume to a cumulative total when the security's price closes up, and subtracting the day's volume when the security's price closes down.

If today's close is greater than yesterday's close then:
OBV = Yesterday’s OBV + Today’s Volume

If today’s close is less than yesterday’s close then:
OBV = Yesterday’s OBV – Today’s Volume

If today’s close is equal to yesterday’s close then:
OBV = Yesterday’s OBV

```{r OnBalanceVolume}
###TSLA
tslaAddOBV = tslaTemp %>% 
  mutate(obv = TTR::OBV(PX_LAST, PX_VOLUME))

###GOOG
googAddOBV = googTemp %>% 
  mutate(obv = TTR::OBV(PX_LAST, PX_VOLUME))

```

#####Chaikin money flow
Description:
Chaikin Money Flow (CMF) developed by Marc Chaikin is a volume-weighted average of accumulation and distribution over a specified period. The standard CMF period is 21 days. The principle behind the Chaikin Money Flow is the nearer the closing price is to the high, the more accumulation has taken place. Conversely, the nearer the closing price is to the low, the more distribution has taken place. If the price action consistently closes above the bar's midpoint on increasing volume, the Chaikin Money Flow will be positive. Conversely, if the price action consistently closes below the bar's midpoint on increasing volume, the Chaikin Money Flow will be a negative value.

How this indicator works:
* A CMF value above the zero line is a sign of strength in the market, and a value below the zero line is a sign of weakness in the market.
* Wait for the CMF to confirm the breakout direction of price action through trend lines or through support and resistance lines. For example, if a price breaks upward through resistance, wait for the CMF to have a positive value to confirm the breakout direction.
* A CMF sell signal occurs when price action develops a higher high into overbought zones, with the CMF diverging with a lower high and beginning to fall.
* A CMF buy signal occurs when price action develops a lower low into oversold zones, with the CMF diverging with a higher low and beginning to rise.

Calculation:
CMF            = n-day Sum of [(((C - L) - (H - C)) / (H - L)) x Vol] / n-day Sum of Vol
cmfNumerator   = n-day Sum of [(((C - L) - (H - C)) / (H - L)) x Vol]
cmfDenominator = n-day Sum of Vol

Where:
n = number of periods, typically 21
H = high
L = low
C = close
Vol = volume

```{r chaikinMoneyFlow}
#TSLA
daysCMF = 21
tslaAddCMF = tslaAddOBV %>%
  mutate(cmf = TTR::CMF(tslaAddOBV[,c("PX_HIGH","PX_LOW","PX_LAST")], PX_VOLUME, n = daysCMF))
#GOOG
daysCMF = 21
googAddCMF = googAddOBV %>%
  mutate(cmf = TTR::CMF(googAddOBV[,c("PX_HIGH","PX_LOW","PX_LAST")], PX_VOLUME, n = daysCMF))
```

#####Exponential Moving Average (EMA)

An exponential moving average (EMA) is a type of moving average (MA) that places a greater weight and significance on the most recent data points. The exponential moving average is also referred to as the exponentially weighted moving average. An exponentially weighted moving average reacts more significantly to recent price changes than a simple moving average (SMA), which applies an equal weight to all observations in the period.

* EMA = Closing price x multiplier + EMA (previous day) x (1-multiplier)

######Limitations of the EMA
It is unclear whether or not more emphasis should be placed on the most recent days in the time period. Many traders believe that new data better reflects the current trend of the security. At the same time, others feel that overweighting recent dates creates a bias that leads to more false alarms.

Similarly, the EMA relies wholly on historical data. Many economists believe that markets are efficient, which means that current market prices already reflect all available information. If markets are indeed efficient, using historical data should tell us nothing about the future direction of asset prices.

EMA calculates an exponentially-weighted mean, giving more weight to recent observations. See
Warning section below.
```{r exponentialMovingAverage}
#TSLA
daysEMA = 10 #EMA lengths
tslaAddEMA = tslaAddCMF %>%
  mutate(emaPrice = TTR::EMA(tslaAddCMF$PX_LAST,daysEMA)) %>%
  mutate(emaVolume = TTR::EMA(tslaAddCMF$PX_VOLUME,daysEMA))

###GOOG
daysEMA = 10 #EMA lengths
googAddEMA = googAddCMF %>%
  mutate(emaPrice = TTR::EMA(googAddCMF$PX_LAST,daysEMA)) %>%
  mutate(emaVolume = TTR::EMA(googAddCMF$PX_VOLUME,daysEMA))
```

```{r relativeStrengthIndex(RSI)}
###Tsla
daysRSI = 14
tslaAddRSI = tslaAddEMA %>%
  mutate(rsi = TTR::RSI(PX_LAST, n=daysRSI, maType="WMA", wts=PX_VOLUME))
###GOOG
daysRSI = 14
googAddRSI = googAddEMA %>%
  mutate(rsi = TTR::RSI(PX_LAST, n=daysRSI, maType="WMA", wts=PX_VOLUME))
```

SMA calculates the arithmetic mean of the series over the past n observations.
```{r simpleMovingAverage(SMA)}
###Tsla
daysSMA = 10
tslaAddSMA = tslaAddRSI %>%
  mutate(smaPrice = TTR::SMA(PX_LAST, n=daysSMA)) %>%
  mutate(smaVolume = TTR::SMA(PX_VOLUME, n=daysSMA))
###GOOG
daysSMA = 10
googAddSMA = googAddRSI %>%
  mutate(smaPrice = TTR::SMA(PX_LAST, n=daysSMA)) %>%
  mutate(smaVolume = TTR::SMA(PX_VOLUME, n=daysSMA))
```

```{r momentumStochasticIndex(K% and D%)}
###Tsla
#Stochastic Momentum Index
daysK = 5 #%K is usually set to 5 and represents the main movements of price – slow line.
daysD = 3 #%D is the fast line, a simple moving average of the %K and is set to 3.
tslaHLC  = tslaSelected %>% dplyr::select(PX_HIGH, PX_LOW, PX_LAST)
tslaAddK = tslaAddSMA %>%
 dplyr::mutate(smi_K = (stoch(tslaHLC, nFastK = daysK, nFastD = daysD, nSlowD = 3, bounded = TRUE,smooth = 1))[,1]) %>% #calculate fastK%
 dplyr::mutate(smi_D = (stoch(tslaHLC, nFastK = daysK, nFastD = daysD, nSlowD = 3, bounded = TRUE,smooth = 1))[,2])  #calculate fastD%

###GOOG
daysK = 5 #%K is usually set to 5 and represents the main movements of price – slow line.
daysD = 3 #%D is the fast line, a simple moving average of the %K and is set to 3.
googHLC  = googSelected %>% dplyr::select(PX_HIGH, PX_LOW, PX_LAST)
googAddK = googAddSMA %>%
 dplyr::mutate(smi_K = (stoch(googHLC, nFastK = daysK, nFastD = daysD, nSlowD = 3, bounded = TRUE,smooth = 1))[,1]) %>% #calculate fastK%
 dplyr::mutate(smi_D = (stoch(googHLC, nFastK = daysK, nFastD = daysD, nSlowD = 3, bounded = TRUE,smooth = 1))[,2])  #calculate fastD%

#bounded:Logical, should current period’s values be used in the calculation? Set it as TRUE here. We will align Y's with past X's in Section xxx.
#Reference:https://library.tradingtechnologies.com/trade/chrt-ti-stochastic-momentum-index.html

```

Moving Average Convergence Divergence
```{r MACD}
###TSLA
daysMACDslow   = 26 #default nSlow
daysMACDfast   = 12 #default nFast
daysMACDsignal = 9  #default nSig

tslaAddMACD = tslaAddK %>%
  mutate(macdPrice = (TTR::MACD(PX_LAST, nFast = daysMACDfast, nSlow = daysMACDslow, nSig = daysMACDsignal))[,1]) %>%
  #mutate(macdPrice = TTR::MACD(nFast = 12, nSlow = 26, nSig = 9, maType="WMA", wts=PX_VOLUME)) %>% # weighted macdPrice
  mutate(macdVolume = (TTR::MACD(PX_VOLUME, nFast = daysMACDfast, nSlow = daysMACDslow, nSig = daysMACDsignal))[,1])

###GOOG
daysMACDslow   = 26 #default nSlow
daysMACDfast   = 12 #default nFast
daysMACDsignal = 9  #default nSig

googAddMACD = googAddK %>%
  mutate(macdPrice = (TTR::MACD(PX_LAST, nFast = daysMACDfast, nSlow = daysMACDslow, nSig = daysMACDsignal))[,1]) %>%
  #mutate(macdPrice = TTR::MACD(nFast = 12, nSlow = 26, nSig = 9, maType="WMA", wts=PX_VOLUME)) %>% # weighted macdPrice
  mutate(macdVolume = (TTR::MACD(PX_VOLUME, nFast = daysMACDfast, nSlow = daysMACDslow, nSig = daysMACDsignal))[,1])
```

```{r LarryWilliam's R%}
###TSLA
daysR = 14 #default
#tslaHLC  = tslaSelected %>% dplyr::select(PX_HIGH, PX_LOW, PX_LAST)
tslaAddR = tslaAddMACD %>%
  mutate(larryWilliamsR = TTR::WPR(tslaHLC, n = daysR))

###GOOG
daysR = 14 #default
#googHLC  = googSelected %>% dplyr::select(PX_HIGH, PX_LOW, PX_LAST)
googAddR = googAddMACD %>%
  mutate(larryWilliamsR = TTR::WPR(googHLC, n = daysR))

```

TheWilliams Accumulation / Distribution (AD) line is a measure of market momentum. Developed by Larry Williams. The Williams AD line differs from OBV and chaikinAD in that it doesn’t take volume into account.
```{r Accumulation/Distribution}
###TSLA
tslaAddAD = tslaAddR %>%
  mutate(williamsAD = TTR::williamsAD(tslaHLC))
###GOOG
googAddAD = googAddR %>%
  mutate(williamsAD = TTR::williamsAD(googHLC))

```

The Commodity Channel Index (CCI) attempts to identify starting and ending trends.CCI relates the current price and the average of price over n periods. The CCI usually falls in a channel of -100 to 100. A basic CCI trading system is: Buy (sell) if CCI rises above 100 (falls below -100) and sell (buy) when it falls below 100 (rises above -100).

```{r CommodityChannelIndex(CCI)}
###TSLA
daysCCI = 20
tslaAddCCI = tslaAddAD %>%
  mutate(cci = TTR::CCI(tslaHLC, n = daysCCI))

###GOOG
daysCCI = 20
googAddCCI = googAddAD %>%
  mutate(cci = TTR::CCI(googHLC, n = daysCCI))

#--End--#

#Summarize how many indicators we've added. 

beforeAfterVarTsla = tslaAddCCI[,(ncol(tslaSelected)+1):ncol(tslaAddCCI)]
names(beforeAfterVarTsla)

beforeAfterVarGoog = googAddCCI[,(ncol(googSelected)+1):ncol(googAddCCI)]
names(beforeAfterVarGoog)
```

```{r additionMeasuresOnStockTrend}
###TSLA
#Calculate stock price trend supported with significant volume increase
tslaAddVolYesterday = addLaggedVarF(tslaAddCCI$PX_VOLUME,tslaAddCCI, "pastVolume", 1)
tslaAddPx_lastYesterday = addLaggedVarF(tslaAddVolYesterday$PX_LAST,tslaAddVolYesterday, "pastLastPrice", 1)

tslaAddVolTemp1 = tslaAddPx_lastYesterday %>%
  mutate(diffVolume = PX_VOLUME - pastVolume1) %>%
  dplyr::slice(-1) %>%
  mutate(increasedVolume = rep(0.01, nrow(.)))

for (i in 1:nrow(tslaAddVolTemp1)) {
  if (tslaAddVolTemp1$diffVolume[i] <=0 ) {
    tslaAddVolTemp1$increasedVolume[i] = 0
  } else {
    tslaAddVolTemp1$increasedVolume[i] = 1*tslaAddVolTemp1$diffVolume[i] 
  }
}


varPastVolumesF = function(featureVolume,lagsize) {
  pastVol_upper = rep(0.0123, length(featureVolume))
  for (d in 1:(length(featureVolume) - lagsize + 1)) {
    pastVol = featureVolume[d:(d + lagsize - 1)]
    pastVol_upper[d+lagsize - 1] = mean(pastVol) + 1.28*var(pastVol)**.5 
    #80% confidence
  }
  return(pastVol_upper)
}

pastVol_upperTsla = varPastVolumesF(tslaAddVolTemp1$PX_VOLUME,5) 
#pastVol_upper[1:20]

tslaAddVolTemp2 = tslaAddVolTemp1 %>%
  mutate(pastVol_upperTsla) %>%
  mutate(afterHourDiffPriceRatio = (PX_OPEN - pastLastPrice1)/pastLastPrice1) %>% 
  #afterHourDiffPrice = closePrice_yesterday - openPrice_today
  mutate(largerThanUpperVol = rep(0.0123, nrow(.)))

#Valid volume increase (when the current volume is larger than the upper bound of volumes in past five days including the current date.)

for (i in 1:nrow(tslaAddVolTemp2)) {
  if (tslaAddVolTemp2$PX_VOLUME[i] < tslaAddVolTemp2$pastVol_upper[i]) {
    tslaAddVolTemp2$largerThanUpperVol[i] = 0
  } else {
    tslaAddVolTemp2$largerThanUpperVol[i] = 1*tslaAddVolTemp2$diffVolume[i] 
  }
}

#names(tslaAddVolTemp2)
#Add interaction terms
tslaAddVolTrend = tslaAddVolTemp2 %>% 
  mutate(increaseTrend = diffLastOpen * increasedVolume) %>%
  mutate(strongIncreaseTrend = diffLastOpen * largerThanUpperVol) %>%
  select(- c(pastVolume1,pastLastPrice1,diffVolume,increasedVolume,pastVol_upperTsla,largerThanUpperVol))



###GOOG
#Calculate stock price trend supported with significant volume increase
googAddVolYesterday = addLaggedVarF(googAddCCI$PX_VOLUME,googAddCCI, "pastVolume", 1)
googAddPx_lastYesterday = addLaggedVarF(googAddVolYesterday$PX_LAST,googAddVolYesterday, "pastLastPrice", 1)

googAddVolTemp1 = googAddPx_lastYesterday %>%
  mutate(diffVolume = PX_VOLUME - pastVolume1) %>%
  dplyr::slice(-1) %>%
  mutate(increasedVolume = rep(0.01, nrow(.)))

for (i in 1:nrow(googAddVolTemp1)) {
  if (googAddVolTemp1$diffVolume[i] <=0 ) {
    googAddVolTemp1$increasedVolume[i] = 0
  } else {
    googAddVolTemp1$increasedVolume[i] = 1*googAddVolTemp1$diffVolume[i] 
  }
}

pastVol_upperGoog = varPastVolumesF(googAddVolTemp1$PX_VOLUME,5) 
#pastVol_upper[1:20]

googAddVolTemp2 = googAddVolTemp1 %>%
  mutate(pastVol_upperGoog) %>%
  mutate(afterHourDiffPriceRatio = (PX_OPEN - pastLastPrice1)/pastLastPrice1) %>% 
  #afterHourDiffPrice = closePrice_yesterday - openPrice_today
  mutate(largerThanUpperVol = rep(0.0123, nrow(.)))

#Valid volume increase (when the current volume is larger than the upper bound of volumes in past five days including the current date.)

for (i in 1:nrow(googAddVolTemp2)) {
  if (googAddVolTemp2$PX_VOLUME[i] < googAddVolTemp2$pastVol_upperGoog[i]) {
    googAddVolTemp2$largerThanUpperVol[i] = 0
  } else {
    googAddVolTemp2$largerThanUpperVol[i] = 1*googAddVolTemp2$diffVolume[i] 
  }
}

#names(googAddVolTemp2)
#Add interaction terms
googAddVolTrend = googAddVolTemp2 %>% 
  mutate(increaseTrend = diffLastOpen * increasedVolume) %>%
  mutate(strongIncreaseTrend = diffLastOpen * largerThanUpperVol) %>%
  select(- c(pastVolume1,pastLastPrice1,diffVolume,increasedVolume,pastVol_upperGoog,largerThanUpperVol))

```

Up till now, we added:
*TSLA stock indicators
`r names(beforeAfterVarTsla)`

*GOOG stock indicators
`r names(beforeAfterVarGoog)`

```{r addLaggedVariablesFunctions}
#Add lagged values of features. 
addLaggedVarF = function(feature, dataForModification, featureName,lagSize ) {
  for (lags in 1:lagSize) { 
    newColName = paste0(featureName, lags)
    dataForModification = dataForModification %>%
      dplyr::mutate(!!sym(newColName) := dplyr::lag(feature, n = lags))
  }
  return(dataForModification)
}


addDiffVarF = function(dataForModification,nColumn) {
  #"tslaSelected" is created to remove features with too many missing values from the original data
  #Purpose: Take the first difference of features in "tslaSelected"
  diffFeature = as_tibble(matrix(rep(0,((nrow(dataForModification)-1)*nColumn)),ncol = nColumn))
  featureName = names(dataForModification)[1:nColumn]
  
  for (j in 1:nColumn) {
    diffFeature[,j] = as_tibble(matrix(diff(dataForModification[,j]), ncol = 1))
    names(diffFeature)[j] = paste0(featureName[j],"_diffPreviousDay")
  }
 diffFeature = lag(diffFeature) #Use the difference between today and yesterday to predict tomorrow; first two rows should be NAs. It will be taken care later with na.omit()
 dataForModification_ = dataForModification[2:nrow(dataForModification),]
 laggedDiff = cbind(dataForModification_,diffFeature)
 laggedDiff %>% transmute()
 return(laggedDiff)
}

#tslaAddDiff =  addDiffVarF(tslaSelected,ncol(tslaSelected))
``` 
A few added indicators represent moving average of price or volume in a past designated period. Hence, we probably don't need to include lagged values. 
Probably delete above codes later ##################

Up till now, we added xx stock indicators:
`r names(beforeAfterVar)`



```{r assignYandX}
###TSLA
tslaY  = tslaAddVolTrend %>%
  na.omit() %>%
  dplyr::select(diffHighLow, stockReturn) %>%
  dplyr::as_tibble() %>%
  dplyr::slice(-1)

tslaX    = tslaAddVolTrend %>%
  na.omit() %>%
  .[1:(nrow(.)-1),] %>%
  dplyr::as_tibble(.) 

names(tslaY)[1] = "diffHighLowTomorrow" #Y1
names(tslaY)[2] = "stockReturnTomorrow" #Y2

tslaFull = dplyr::as_tibble(cbind(tslaY,tslaX))

#Correlation between stock data of today and difference between highest and lowest stock prices on tomorrow is much higher than correlation between stock data of today and daily stock return on tomorrow. Thus, it's suggested to use "diffHighLowTomorrow " as the supervisor in model fitting.

#summary(tslaY$diffHighLowTomorrow)

##GOOG
googY  = googAddVolTrend %>%
  na.omit() %>%
  dplyr::select(diffHighLow, stockReturn) %>%
  dplyr::as_tibble() %>%
  dplyr::slice(-1)


googX    = googAddVolTrend %>%
  na.omit() %>%
  .[1:(nrow(.)-1),] %>%
  dplyr::as_tibble(.) 

names(googY)[1] = "diffHighLowTomorrow" #Y1
names(googY)[2] = "stockReturnTomorrow" #Y2

googFull = dplyr::as_tibble(cbind(googY,googX))

```

```{r PCA Regression}
###TSLA
#It's suggested to use only quantitative variables in PCA regression
#Transform the selected data

(tslaXTrans = caret::preProcess(as.data.frame(tslaX), method = c("BoxCox","center","scale","pca")))

#Apply the transformations("BoxCox","center","scale","pca")
pcaTslaX = stats::predict(tslaXTrans,as.data.frame(tslaX)) #Outputs are PCA components
#head(pcaTslaX)

#Summary of PCA results
tslaXTrans_     = caret::preProcess(as.data.frame(tslaX), method = c("BoxCox","center","scale"))
propVar_pcaTslaX = as.data.frame(tslaX) %>%
                 stats::predict(tslaXTrans_,.) %>%
                 stats::prcomp() %>%
                 summary()
propVar_pcaTslaX #proportion of variance explained by each PCs

#Check extreme observations via PCA

ggplot(data = pcaTslaX)+
  geom_point(aes(x = PC1, y= PC2))

ggplot(data = pcaTslaX)+
  geom_point(aes(x = PC3, y= PC2))

ggplot(data = pcaTslaX)+
  geom_point(aes(x = PC1, y= PC3))

plot3D::scatter3D(pcaTslaX$PC1, pcaTslaX$PC2, pcaTslaX$PC3, pch = 16,  
                  theta = 20, phi = 20, main = "PCA - Tesla", xlab = "PC1", 
                  ylab ="PC2", zlab = "PC3", bty = "g", ticktype = "detailed")
#One outlier. 

##diffHighLow vs PCs
# PC1 (better result for classification)
ggplot(data = pcaTslaX)+
  geom_point(aes(x = PC1, y=tslaY$diffHighLowTomorrow), color=ifelse(tslaY$diffHighLowTomorrow > mean(tslaY$diffHighLowTomorrow),"red", "blue"))

ggplot(data = pcaTslaX)+
  geom_point(aes(x = PC1, y=tslaY$diffHighLowTomorrow)
, color=ifelse(tslaX$diffHighLow > mean(tslaX$diffHighLow),"red", "blue"))

# PC2 (impossible to classify Y)
ggplot(data = pcaTslaX)+
  geom_point(aes(x = PC2, y=tslaY$diffHighLowTomorrow), color=ifelse(tslaY$diffHighLowTomorrow > mean(tslaY$diffHighLowTomorrow),"red", "blue"))

ggplot(data = pcaTslaX)+
  geom_point(aes(x = PC2, y=tslaY$diffHighLowTomorrow)
, color=ifelse(tslaX$diffHighLow > mean(tslaX$diffHighLow),"red", "blue"))

# PC3 (impossible to classify Y)
ggplot(data = pcaTslaX)+
  geom_point(aes(x = PC3, y=tslaY$diffHighLowTomorrow)
             , color=ifelse(tslaY$diffHighLowTomorrow > mean(tslaY$diffHighLowTomorrow),"red", "blue"))

ggplot(data = pcaTslaX)+
  geom_point(aes(x = PC3, y=tslaY$diffHighLowTomorrow)
, color=ifelse(tslaX$diffHighLow > mean(tslaX$diffHighLow),"red", "blue"))

# 3D Plot(PC1, PC3, diffHighLowTomorrow)
############################!!!!!!!!!!add legends for colors
scatterplot3d::scatterplot3d(x = pcaTslaX$PC1, y = pcaTslaX$PC2,
                             z = tslaY$diffHighLowTomorrow, pch = 16, color = ifelse(tslaY$diffHighLowTomorrow > mean(tslaY$diffHighLowTomorrow),"red", "blue"))
############################!!!!!!!!!!add legends for colors
scatterplot3d::scatterplot3d(x = pcaTslaX$PC1, y = pcaTslaX$PC2,
                             z = tslaY$diffHighLowTomorrow, pch = 16, color = ifelse(tslaX$diffHighLow > mean(tslaX$diffHighLow),"red", "blue"))

##stockReturn vs PCs
# PC1 (Impossible to classify Y)
ggplot(data = pcaTslaX)+
  geom_point(aes(x = PC1, y=tslaY$stockReturnTomorrow), color=ifelse(tslaX$stockReturn > mean(tslaX$stockReturn),"red", "blue"))

# PC2 (Better)
ggplot(data = pcaTslaX)+
  geom_point(aes(x = PC2, y=tslaY$stockReturnTomorrow), color=ifelse(tslaX$stockReturn > mean(tslaX$stockReturn),"red", "blue"))
# PC3 (Better)
ggplot(data = pcaTslaX)+
  geom_point(aes(x = PC3, y=tslaY$stockReturnTomorrow), color=ifelse(tslaX$stockReturn > mean(tslaX$stockReturn),"red", "blue"))





###GOOG

#Transform the selected data
(googXTrans = caret::preProcess(as.data.frame(googX), method = c("BoxCox","center","scale","pca")))

#Apply the transformations("BoxCox","center","scale","pca")
pcaGoogX = stats::predict(googXTrans,as.data.frame(googX)) #Outputs are PCA components
#head(pcaTslaX)

#Summary of PCA results
googXTrans_     = caret::preProcess(as.data.frame(googX), method = c("BoxCox","center","scale"))
propVar_pcaGoogX = as.data.frame(googX) %>%
                 stats::predict(googXTrans_,.) %>%
                 stats::prcomp() %>%
                 summary()
propVar_pcaGoogX #proportion of variance explained by each PCs

#Check extreme observations via PCA

ggplot(data = pcaGoogX)+
  geom_point(aes(x = PC1, y= PC2))

ggplot(data = pcaGoogX)+
  geom_point(aes(x = PC3, y= PC2))

ggplot(data = pcaGoogX)+
  geom_point(aes(x = PC1, y= PC3))

plot3D::scatter3D(pcaGoogX$PC1, pcaGoogX$PC2, pcaGoogX$PC3, pch = 16,  
                  theta = 20, phi = 20, main = "PCA - GOOG", xlab = "PC1", 
                  ylab ="PC2", zlab = "PC3", bty = "g", ticktype = "detailed")
#Two or a few more outliers. 

##diffHighLow vs PCs
# PC1 (better result for classification)
ggplot(data = pcaGoogX)+
  geom_point(aes(x = PC1, y=googY$diffHighLowTomorrow), color=ifelse(googY$diffHighLowTomorrow > mean(googY$diffHighLowTomorrow),"red", "blue"))

ggplot(data = pcaGoogX)+
  geom_point(aes(x = PC1, y=googY$diffHighLowTomorrow)
, color=ifelse(googX$diffHighLow > mean(googX$diffHighLow),"red", "blue"))

# PC2 (impossible to classify Y)
ggplot(data = pcaGoogX)+
  geom_point(aes(x = PC2, y=googY$diffHighLowTomorrow), color=ifelse(googY$diffHighLowTomorrow > mean(googY$diffHighLowTomorrow),"red", "blue"))

ggplot(data = pcaGoogX)+
  geom_point(aes(x = PC2, y=googY$diffHighLowTomorrow)
, color=ifelse(googX$diffHighLow > mean(googX$diffHighLow),"red", "blue"))

# PC3 (impossible to classify Y)
ggplot(data = pcaGoogX)+
  geom_point(aes(x = PC3, y=googY$diffHighLowTomorrow)
             , color=ifelse(googY$diffHighLowTomorrow > mean(googY$diffHighLowTomorrow),"red", "blue"))

ggplot(data = pcaGoogX)+
  geom_point(aes(x = PC3, y=googY$diffHighLowTomorrow)
, color=ifelse(googX$diffHighLow > mean(googX$diffHighLow),"red", "blue"))

# 3D Plot(PC1, PC3, diffHighLowTomorrow)

############################!!!!!!!!!!add legends for colors
scatterplot3d::scatterplot3d(x = pcaGoogX$PC1, y = pcaGoogX$PC2,
                             z = googY$diffHighLowTomorrow, pch = 16, color = ifelse(googY$diffHighLowTomorrow > mean(googY$diffHighLowTomorrow),"red", "blue"))

############################!!!!!!!!!!add legends for colors
scatterplot3d::scatterplot3d(x = pcaGoogX$PC1, y = pcaGoogX$PC2,
                             z = googY$diffHighLowTomorrow, pch = 16, color = ifelse(googX$diffHighLow > mean(googX$diffHighLow),"red", "blue"))


##stockReturn vs PCs
# PC1 (Impossible to classify Y)
ggplot(data = pcaGoogX)+
  geom_point(aes(x = PC1, y=googY$stockReturnTomorrow), color=ifelse(googX$stockReturn > mean(googX$stockReturn),"red", "blue"))

# PC2 (Better)
ggplot(data = pcaGoogX)+
  geom_point(aes(x = PC2, y=googY$stockReturnTomorrow), color=ifelse(googX$stockReturn > mean(googX$stockReturn),"red", "blue"))
# PC3 (Better)
ggplot(data = pcaGoogX)+
  geom_point(aes(x = PC3, y=googY$stockReturnTomorrow), color=ifelse(googX$stockReturn > mean(googX$stockReturn),"red", "blue"))

```

##### 5.2 Linear Model
```{r dummyVarForLogisticRegression}
#hist(tslaReg$stockReturn, breaks=20, freq=FALSE,main = paste("Density Plot - ",names(tslaReg$stockReturn)),xlab = " ") # stock return is normally distributed.

###TSLA
XtslaAddLevels = tslaX %>%
  mutate(aboveR1 = factor(ifelse(PX_HIGH > R1, "Yes", "No"))) %>%
  mutate(aboveR2 = factor(ifelse(PX_HIGH > R2, "Yes", "No"))) %>%
  mutate(aboveR3 = factor(ifelse(PX_HIGH > R3, "Yes", "No"))) %>%
  mutate(belowS1 = factor(ifelse(PX_LOW  < S1, "Yes", "No"))) %>%
  mutate(belowS2 = factor(ifelse(PX_LOW  < S2, "Yes", "No"))) %>%
  mutate(belowS3 = factor(ifelse(PX_LOW  < S3, "Yes", "No"))) %>%
  select(aboveR1, aboveR2, aboveR3, belowS1, belowS2, belowS3)            
XtslaDummyModel = caret::dummyVars(~ ., data = XtslaAddLevels, fullRank = TRUE)

YtslaAddLevels = as_tibble(factor(ifelse(tslaY$stockReturnTomorrow > 0, "Increased", "Decreased"))) 
YtslaDummyModel = caret::dummyVars(~ ., data = YtslaAddLevels, fullRank = TRUE)

XtslaDummy = as_tibble(predict(XtslaDummyModel, XtslaAddLevels))
YtslaDummy = as_tibble(predict(YtslaDummyModel, YtslaAddLevels))

names(YtslaDummy)[1] = "stockReturnTomorrowDummy" #Y2

tslaFullWithDummy = tslaFull %>% 
  mutate(XtslaDummy) %>% 
  dplyr::select(-R1,-S1,-R2,-S2,-R3,-S3) %>% 
  #Drop PP, R1, R2, R3, S1, S2, S3 
  mutate(YtslaDummy) 

XtslaWithDummy = tslaX %>%
  mutate(XtslaDummy) %>% 
  dplyr::select(-R1,-S1,-R2,-S2,-R3,-S3)

YtslaWithDummy = tslaY %>% mutate(YtslaDummy) 





###GOOG
XgoogAddLevels = googX %>%
  mutate(aboveR1 = factor(ifelse(PX_HIGH > R1, "Yes", "No"))) %>%
  mutate(aboveR2 = factor(ifelse(PX_HIGH > R2, "Yes", "No"))) %>%
  mutate(aboveR3 = factor(ifelse(PX_HIGH > R3, "Yes", "No"))) %>%
  mutate(belowS1 = factor(ifelse(PX_LOW  < S1, "Yes", "No"))) %>%
  mutate(belowS2 = factor(ifelse(PX_LOW  < S2, "Yes", "No"))) %>%
  mutate(belowS3 = factor(ifelse(PX_LOW  < S3, "Yes", "No"))) %>%
  select(aboveR1, aboveR2, aboveR3, belowS1, belowS2, belowS3)            
XgoogDummyModel = caret::dummyVars(~ ., data = XgoogAddLevels, fullRank = TRUE)

YgoogAddLevels = as_tibble(factor(ifelse(googY$stockReturnTomorrow > 0, "Increased", "Decreased"))) 
YgoogDummyModel = caret::dummyVars(~ ., data = YgoogAddLevels, fullRank = TRUE)

XgoogDummy = as_tibble(predict(XgoogDummyModel, XgoogAddLevels))
YgoogDummy = as_tibble(predict(YgoogDummyModel, YgoogAddLevels))

names(YgoogDummy)[1] = "stockReturnTomorrowDummy" #Y2

googFullWithDummy = googFull %>% 
  mutate(XgoogDummy) %>% 
  dplyr::select(-R1,-S1,-R2,-S2,-R3,-S3) %>% 
  #Drop PP, R1, R2, R3, S1, S2, S3 
  mutate(YgoogDummy) 

XgoogWithDummy = googX %>%
  mutate(XgoogDummy) %>% 
  dplyr::select(-R1,-S1,-R2,-S2,-R3,-S3)

YgoogWithDummy = googY %>% mutate(YgoogDummy) 

```

```{r reviewHighCorr}
###TSLA
hclustPlotF(tslaFullWithDummy)
highCorrTsla = highCorrF(XtslaWithDummy,0.8)
highCorrTsla

XtslaWithDummyRemoveCorr = XtslaWithDummy %>% select(-highCorrTsla)
hclustPlotF(XtslaWithDummyRemoveCorr)

###GOOG
hclustPlotF(googFullWithDummy)
highCorrGoog = highCorrF(XgoogWithDummy,0.8)
highCorrGoog

XgoogWithDummyRemoveCorr = XgoogWithDummy %>% select(-highCorrGoog)
hclustPlotF(XgoogWithDummyRemoveCorr)

```

```{r linearRegression}
###TSLA
#First, remove highly correlated features and correct skewness
highCorrTsla = highCorrF(tslaX,0.8)
tslaXRemoveCorr = tslaX %>% select(-highCorrTsla)

tslaXyeoJ = tslaXRemoveCorr %>%
  caret::preProcess(method = "YeoJohnson") %>%
  predict(newdata = tslaXRemoveCorr)

#hist(tslaY$diffHighLowTomorrow)
#hist(tslaY$stockReturnTomorrow)

tslaYyeoJ = tslaY %>%
  caret::preProcess(method = "YeoJohnson") %>%
  predict(newdata = tslaY)
#hist(tslaYyeoJ$diffHighLowTomorrow)
#hist(tslaYyeoJ$stockReturnTomorrow)

Ktsla = 10
cvIndexTsla = caret::createFolds(tslaYyeoJ$diffHighLowTomorrow, k = Ktsla, returnTrain = TRUE)
trControl = trainControl(method = "cv", index = cvIndexTsla)

#names(tslaXyeoJ)
XtslaLinearReg = tslaXyeoJ %>%
  caret::preProcess(method = c("center","scale")) %>%
  predict(tslaXyeoJ) %>%
  cbind(.,XtslaDummy)

#names(XtslaDummy)

YtslaLinearReg = tslaYyeoJ %>%
  caret::preProcess(method = c("center","scale")) %>%
  predict(newdata = tslaYyeoJ) %>%
  cbind(.,YtslaDummy)

#Linear regression when Supervisor = diffHighLowTomorrow
lmOutTslaDiffHighLow = train(x = XtslaLinearReg, y = YtslaLinearReg$diffHighLowTomorrow, method = "lm", trControl = trControl)

lmOutTslaDiffHighLow

#Linear regression when Supervisor = StockReturn
lmOutTslaStockReturn = train(x = XtslaLinearReg, y = YtslaLinearReg$stockReturnTomorrow, method = "lm", trControl = trControl)

lmOutTslaStockReturn









###GOOG
#First, correct skewness
highCorrGoog = highCorrF(googX,0.8)
googXRemoveCorr = googX %>% select(-highCorrGoog)

googXyeoJ = googXRemoveCorr %>%
  caret::preProcess(method = "YeoJohnson") %>%
  predict(newdata = googXRemoveCorr)

googYyeoJ = googY %>%
  caret::preProcess(method = "YeoJohnson") %>%
  predict(newdata = googY)


Kgoog = 10
cvIndexGoog = caret::createFolds(googYyeoJ$diffHighLowTomorrow, k = Kgoog, returnTrain = TRUE)
trControl = trainControl(method = "cv", index = cvIndexGoog)

XgoogLinearReg = googXyeoJ %>%
  caret::preProcess(method = c("center","scale")) %>%
  predict(googXyeoJ) %>%
  cbind(.,XgoogDummy) 

YgoogLinearReg = googYyeoJ %>%
  caret::preProcess(method = c("center","scale")) %>%
  predict(newdata = googYyeoJ) %>%
  cbind(.,YgoogDummy) 

#Linear regression when Supervisor = diffHighLowTomorrow
lmOutGoogDiffHighLow = train(x = XgoogLinearReg, y = YgoogLinearReg$diffHighLowTomorrow, method = "lm", trControl = trControl)

lmOutGoogDiffHighLow

#Linear regression when Supervisor = StockReturn
lmOutGoogStockReturn = train(x = XgoogLinearReg, y = YgoogLinearReg$stockReturnTomorrow, method = "lm", trControl = trControl)

lmOutGoogStockReturn
```

```{r linearRegPenalized}
###TSLA
#ridge regression
trControl = trainControl(method = "cv", index = cvIndexTsla)
ridgeOutTsla = train(x = XtslaLinearReg, y = YtslaLinearReg$diffHighLowTomorrow, method = "ridge", tuneLength = 10,trControl = trControl)

ridgeOutTsla

#lasso regression
lassoOutTsla = train(x = XtslaLinearReg, y = YtslaLinearReg$diffHighLowTomorrow, method = "lasso", tuneLength = 10,trControl = trControl)

lassoOutTsla

#elastic net
gridLambdaAlpha = expand.grid(lambda = seq(1e-6, 0.05, length = 15), alpha = c(0,0.25,.5,.75,1))

enetOutTsla = train(x = XtslaLinearReg, y = YtslaLinearReg$diffHighLowTomorrow,
                method = "glmnet", tuneGrid = gridLambdaAlpha, trControl = trControl)

enetOutTsla

plot(enetOutTsla, xlab = "Lambda (Elastic Net Penalty)", ylab = "RMSE (K-fold CV)" )

#Results of penalized linear model
#names(enetOutTsla$finalModel)

alphaHatTsla  = enetOutTsla$finalModel$tuneValue$alpha
lambdaHatTsla = enetOutTsla$finalModel$tuneValue$lambda




###GOOG
#ridge regression
trControl = trainControl(method = "cv", index = cvIndexGoog)
ridgeOutGoog = train(x = XgoogLinearReg, y = YgoogLinearReg$diffHighLowTomorrow, method = "ridge", tuneLength = 10,trControl = trControl)

ridgeOutGoog

#lasso regression
lassoOutGoog = train(x = XgoogLinearReg, y = YgoogLinearReg$diffHighLowTomorrow, method = "lasso", tuneLength = 10,trControl = trControl)

lassoOutGoog

#elastic net
gridLambdaAlpha = expand.grid(lambda = seq(1e-6, 0.05, length = 15), alpha = c(0,0.25,.5,.75,1))

enetOutGoog = train(x = XgoogLinearReg, y = YgoogLinearReg$diffHighLowTomorrow,
                method = "glmnet", tuneGrid = gridLambdaAlpha, trControl = trControl)

enetOutGoog

plot(enetOutGoog, xlab = "Lambda (Elastic Net Penalty)", ylab = "RMSE (K-fold CV)" )

#Results of penalized linear model
#names(enetOutTsla$finalModel)

alphaHatGoog  = enetOutGoog$finalModel$tuneValue$alpha
lambdaHatGoog = enetOutGoog$finalModel$tuneValue$lambda


```

>Tesla (TSLA)

The CV - minimizing values are alpha = `r alphaHatTsla` and lambda = `r lambdaHatTsla`.
Next, refit the model with this alpha by calling "glmnet" directly and select the lambdaHat that minimized CV. 

Use results from  Elastic Net Regression to find coefficients Beta's.

```{r betaPenalizedRegression}
###TSLA
glmnetOutTsla = glmnet::glmnet(x = as.matrix(XtslaLinearReg), y = YtslaLinearReg$diffHighLowTomorrow, alpha = alphaHatTsla, standardize = FALSE)
(betaHatTsla = coef(glmnetOutTsla, s = lambdaHatTsla))

###GOOG
glmnetOutGoog = glmnet::glmnet(x = as.matrix(XgoogLinearReg), y = YgoogLinearReg$diffHighLowTomorrow, alpha = alphaHatGoog, standardize = FALSE)
(betaHatGoog = coef(glmnetOutTsla, s = lambdaHatTsla))

```

##### 5.3 Nonlinear Model (MARS)

```{r dataSplit}
set.seed(2020)
###TSLA
YtslaNN    = YtslaLinearReg #highly correlated features are removed; standardized
trainIndex = caret::createDataPartition(YtslaNN$diffHighLowTomorrow, p = 0.25, list = FALSE)

YtslaTrain_DiffHighLow      = YtslaNN$diffHighLowTomorrow[trainIndex]
YtslaTrain_StockReturn      = YtslaNN$stockReturnTomorrow[trainIndex]
YtslaTrain_StockReturnDummy = YtslaNN$stockReturnTomorrowDummy[trainIndex]

YtslaTest_DiffHighLow       = YtslaNN$diffHighLowTomorrow[-trainIndex]
YtslaTest_StockReturn       = YtslaNN$stockReturnTomorrow[-trainIndex]
YtslaTest_StockReturnDummy  = YtslaNN$stockReturnTomorrowDummy [-trainIndex]

XtslaNN    = XtslaLinearReg #highly correlated features are removed; standardized

XtslaTrain = XtslaNN[trainIndex,] 
XtslaTest  = XtslaNN[-trainIndex,]

###GOOG
set.seed(2021)
YgoogNN    = YgoogLinearReg #highly correlated features are removed; standardized
trainIndex = caret::createDataPartition(YgoogNN$diffHighLowTomorrow, p = 0.25, list = FALSE)

YgoogTrain_DiffHighLow      = YgoogNN$diffHighLowTomorrow[trainIndex]
YgoogTrain_StockReturn      = YgoogNN$stockReturnTomorrow[trainIndex]
YgoogTrain_StockReturnDummy = YgoogNN$stockReturnTomorrowDummy[trainIndex]

YgoogTest_DiffHighLow       = YgoogNN$diffHighLowTomorrow[-trainIndex]
YgoogTest_StockReturn       = YgoogNN$stockReturnTomorrow[-trainIndex]
YgoogTest_StockReturnDummy  = YgoogNN$stockReturnTomorrowDummy [-trainIndex]

XgoogNN    = XgoogLinearReg

XgoogTrain = XgoogNN[trainIndex,] 
XgoogTest  = XgoogNN[-trainIndex,]
```


```{r MARS}
#Lecture 27
###TSLA
tuneGridTsla = expand.grid(degree = 1:3,
                           nprune = c(10, 15, 20, 25, 30, 40, 50))

set.seed(2020)
number = 5 

#cross validated model

#-1-# Supervisor = StockReturn
marsTuneTslaSR = caret::train(
  x = XtslaTrain,
  y = YtslaTrain_StockReturn,
  method = "earth",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = number),
  tuneGrid = tuneGridTsla
)
marsTuneTslaSR$bestTune
min(marsTuneTslaSR$results$RMSE)
max(marsTuneTslaSR$results$Rsquared)

#library(digest) #Fixed error: Error: package ‘digest’ does not have a namespace
ggplot2::ggplot(marsTuneTslaSR)

#Multiple Linear Regression
mlrCVTslaSR = train(
  x = XtslaTrain,
  y = YtslaTrain_StockReturn,
  method = "lm",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = number)
  #We've standardize our data. 
)

mlrCVTslaSR$results$RMSE
mlrCVTslaSR$results$Rsquared

#Elastic Net
glmnetCVTslaSR = train(
  x = XtslaTrain,
  y = YtslaTrain_StockReturn,
  method = "glmnet",
  trControl = trainControl(method = "cv", number = number),
  metric = "RMSE",
  preProcess = "zv",
  tuneLength = 20
)

min(glmnetCVTslaSR$results$RMSE)
max(glmnetCVTslaSR$results$Rsquared, na.rm = TRUE)




#-2-#Supervisor = diffHighLow
marsTuneTslaHL = caret::train(
  x = XtslaTrain,
  y = YtslaTrain_DiffHighLow,
  method = "earth",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = number),
  tuneGrid = tuneGridTsla
)

marsTuneTslaHL$bestTune
min(marsTuneTslaHL$results$RMSE)
max(marsTuneTslaHL$results$Rsquared, na.rm = TRUE)

ggplot2::ggplot(marsTuneTslaHL)

mlrCVTslaHL = train(
  x = XtslaTrain,
  y = YtslaTrain_DiffHighLow,
  method = "lm",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = number)
  #We've standardize our data. 
)

mlrCVTslaHL$results$RMSE
mlrCVTslaHL$results$Rsquared

glmnetCVTslaHL = train(
  x = XtslaTrain,
  y = YtslaTrain_DiffHighLow,
  method = "glmnet",
  trControl = trainControl(method = "cv", number = number),
  metric = "RMSE",
  preProcess = "zv",
  tuneLength = 20
)

min(glmnetCVTslaHL$results$RMSE)
max(glmnetCVTslaHL$results$Rsquared, na.rm = TRUE)


#Variable Importance
##Supervisor = StockReturn
marsVipTslaSR = vip::vip(marsTuneTslaSR, num_features = 47,  bar = NULL,geom = "point", value = "gcv")+ ggplot2::ggtitle("GCV - TSLA (47 Features)")
plot(marsVipTslaSR)
##Supervisor = diffHighLoW
marsVipTslaHL = vip::vip(marsTuneTslaHL, num_features = 47,  bar = NULL,geom = "point", value = "gcv")+ ggplot2::ggtitle("GCV - TSLA (47 Features)")
plot(marsVipTslaHL)





###GOOG
tuneGridGoog = expand.grid(degree = 1:3,
                           nprune = c(10, 15, 20, 25, 30, 40, 50))

set.seed(2021)
number = 5 

#cross validated model

#-1-# Supervisor = StockReturn
marsTuneGoogSR = caret::train(
  x = XgoogTrain,
  y = YgoogTrain_StockReturn,
  method = "earth",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = number),
  tuneGrid = tuneGridGoog
)
marsTuneGoogSR$bestTune
min(marsTuneGoogSR$results$RMSE)
max(marsTuneGoogSR$results$Rsquared)

#library(digest) #Fixed error: Error: package ‘digest’ does not have a namespace
ggplot2::ggplot(marsTuneGoogSR)

#Multiple Linear Regression
mlrCVGoogSR = train(
  x = XgoogTrain,
  y = YgoogTrain_StockReturn,
  method = "lm",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = number)
  #We've standardize our data. 
)

mlrCVGoogSR$results$RMSE
mlrCVGoogSR$results$Rsquared

#Elastic Net
glmnetCVGoogSR = train(
  x = XgoogTrain,
  y = YgoogTrain_StockReturn,
  method = "glmnet",
  trControl = trainControl(method = "cv", number = number),
  metric = "RMSE",
  preProcess = "zv",
  tuneLength = 20
)

min(glmnetCVGoogSR$results$RMSE)
max(glmnetCVGoogSR$results$Rsquared, na.rm = TRUE)


#-2-#Supervisor = diffHighLow
marsTuneGoogHL = caret::train(
  x = XgoogTrain,
  y = YgoogTrain_DiffHighLow,
  method = "earth",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = number),
  tuneGrid = tuneGridGoog
)

marsTuneGoogHL$bestTune
min(marsTuneGoogHL$results$RMSE)
max(marsTuneGoogHL$results$Rsquared, na.rm = TRUE)

ggplot2::ggplot(marsTuneGoogHL)

mlrCVGoogHL = train(
  x = XgoogTrain,
  y = YgoogTrain_DiffHighLow,
  method = "lm",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = number)
  #We've standardize our data. 
)

mlrCVGoogHL$results$RMSE
mlrCVGoogHL$results$Rsquared

glmnetCVGoogHL = train(
  x = XgoogTrain,
  y = YgoogTrain_DiffHighLow,
  method = "glmnet",
  trControl = trainControl(method = "cv", number = number),
  metric = "RMSE",
  preProcess = "zv",
  tuneLength = 20
)

min(glmnetCVGoogHL$results$RMSE)
max(glmnetCVGoogHL$results$Rsquared, na.rm = TRUE)


#Variable Importance
##Supervisor = StockReturn
marsVipGoogSR = vip::vip(marsTuneGoogSR, num_features = 41,  bar = NULL,geom = "point", value = "gcv")+ ggplot2::ggtitle("GCV - GOOG (47 Features)")
plot(marsVipGoogSR)
##Supervisor = diffHighLoW
marsVipGoogHL = vip::vip(marsTuneGoogHL, num_features = 41,  bar = NULL,geom = "point", value = "gcv")+ ggplot2::ggtitle("GCV - GOOG (47 Features)")
plot(marsVipGoogHL)
```



``` {r summary(MARS vs MLR vs ElasticNet}
#Overall Comparisons of results from previous section

###TSLA
namesTslaRMSE = c("RMSE_MARS_tslaStockReturn",
              "RMSE_MLR_tslaStockReturn",
              "RMSE_GLMNET_tslaStockReturn",
              "RMSE_MARS_tslaDiffHighLow",
              "RMSE_MLR_tslaDiffHighLow",
              "RMSE_GLMNET_tslaDiffHighLow"
              )

summaryTslaRMSE = c(min(marsTuneTslaSR$results$RMSE),
                mlrCVTslaSR$results$RMSE,
                min(glmnetCVTslaSR$results$RMSE),
                min(marsTuneTslaHL$results$RMSE),
                mlrCVTslaHL$results$RMSE,
                min(glmnetCVTslaHL$results$RMSE)
                )

sumTslaRMSE = cbind(namesTslaRMSE,round(summaryTslaRMSE,digits = 5))


knitr::kable(
 sumTslaRMSE,
 col.names = c("Metric","Values"),
 caption   = "Table of RMSE, Tsla ",
 align     = "lccrr"
)


namesTslaRsquared = c("Rsquared_MARS_tslaStockReturn",
              "Rsquared_MLR_tslaStockReturn",
              "Rsquared_GLMNET_tslaStockReturn",
              "Rsquared_MARS_tslaDiffHighLow",
              "Rsquared_MLR_tslaDiffHighLow",
              "Rsquared_GLMNET_tslaDiffHighLow"
              )

summaryTslaRsquared = c(max(marsTuneTslaSR$results$Rsquared),
                mlrCVTslaSR$results$Rsquared,
                max(glmnetCVTslaSR$results$Rsquared, na.rm = TRUE),
                max(marsTuneTslaHL$results$Rsquared),
                mlrCVTslaHL$results$Rsquared,
                max(glmnetCVTslaHL$results$Rsquared, na.rm = TRUE))


sumTslaRsquared = cbind(namesTslaRsquared, round(summaryTslaRsquared,digits = 5))

knitr::kable(
 sumTslaRsquared,
 col.names = c("Metric","Values"),
 caption   = "Table of Rsquared, Tsla ",
 align     = "lccrr"
)




###GOOG
namesGoogRMSE = c("RMSE_MARS_googStockReturn",
              "RMSE_MLR_googStockReturn",
              "RMSE_GLMNET_googStockReturn",
              "RMSE_MARS_googDiffHighLow",
              "RMSE_MLR_googDiffHighLow",
              "RMSE_GLMNET_googDiffHighLow"
              )

summaryGoogRMSE = c(min(marsTuneGoogSR$results$RMSE),
                  mlrCVGoogSR$results$RMSE,
                  min(glmnetCVGoogSR$results$RMSE),
                  min(marsTuneGoogHL$results$RMSE),
                  mlrCVGoogHL$results$RMSE,
                  min(glmnetCVGoogHL$results$RMSE)
                  )

sumGoogRMSE = cbind(namesGoogRMSE,round(summaryGoogRMSE,digits = 5))


knitr::kable(
 sumGoogRMSE,
 col.names = c("Metric","Values"),
 caption   = "Table of RMSE, GOOG ",
 align     = "lccrr"
)


namesGoogRsquared = c("Rsquared_MARS_googStockReturn",
              "Rsquared_MLR_googStockReturn",
              "Rsquared_GLMNET_googStockReturn",
              "Rsquared_MARS_googDiffHighLow",
              "Rsquared_MLR_googDiffHighLow",
              "Rsquared_GLMNET_googDiffHighLow"
              )

summaryGoogRsquared = c(max(marsTuneGoogSR$results$Rsquared),
                mlrCVGoogSR$results$Rsquared,
                max(glmnetCVGoogSR$results$Rsquared, na.rm = TRUE),
                max(marsTuneGoogHL$results$Rsquared),
                mlrCVGoogHL$results$Rsquared,
                max(glmnetCVGoogHL$results$Rsquared, na.rm = TRUE))


sumGoogRsquared = cbind(namesGoogRsquared, round(summaryGoogRsquared,digits = 5))

knitr::kable(
 sumGoogRsquared,
 col.names = c("Metric","Values"),
 caption   = "Table of Rsquared, GOOG ",
 align     = "lccrr"
)

```

```{r createCSVForNN_Python}
#Supervisors and features are all standardized except the dummy variables. 
tslaY1 = YtslaNN$stockReturnTomorrow 
tslaY2 = YtslaNN$stockReturnTomorrowDummy
tslaY3 = YtslaNN$diffHighLowTomorrow

#Supervisors
write.csv(tslaY1,"D:/Mia/STAT/656/tslaSR_Y1.csv", 
          row.names = FALSE)
write.csv(tslaY2,"D:/Mia/STAT/656/tslaSRdummy_Y2.csv", 
          row.names = FALSE)
write.csv(tslaY3,"D:/Mia/STAT/656/tslaHL_Y3.csv", 
          row.names = FALSE)
#Feature Matrix
write.csv(XtslaNNFull,"D:/Mia/STAT/656/XtslaNNFull.csv", 
          row.names = FALSE)

#Feature Matrix without highly correlated features
write.csv(XtslaNNFullRemoveCorr,"D:/Mia/STAT/656/XtslaNNFullRemoveCorr.csv",
          row.names = FALSE)

hclustPlotF(XtslaNNScaled)
```


```{r modelPerformance}
#Calculate test errors

#confusion matrix

```

##### 6 - Neural Networks
```{r scatterPlots}

remove.packages("keras")
remove.packages("tensorflow")

install.packages("reticulate")
library(reticulate)
install.packages("tensorflow")
install.packages("keras")

library(tensorflow)

install_tensorflow()

ncol(XtslaTrain)
tslaNNFit = keras::keras_model_sequential() %>%
  keras::layer_flatten() %>%
  keras::layer_dense(units = 16,activation = "relu", name = "L1") %>%
  keras::layer_dense(units = 16,activation = "relu", name = "L2") %>%
  keras::layer_dense(units = 2, activation = "softmax", name = "output")


tslaNNFit = keras::keras_model_sequential() %>%
  XtslaNNScaled %>%
  layer_flatten() %>%
  keras::layer_dense(units = 16,activation = "relu", name = "L1") %>%
  keras::layer_dense(units = 16,activation = "relu", name = "L2") %>%
  keras::layer_dense(units = 2, activation = "softmax", name = "output")

tensorflow.keras
```

##### 7 - SVM


##### 8 - Random Forest